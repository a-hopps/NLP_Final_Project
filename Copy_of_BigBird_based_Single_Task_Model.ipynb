{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nnpblpm7GVu_",
    "outputId": "0235e25f-2dbd-47eb-aaea-2e8210630830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "#!pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIzql88DGdBT",
    "outputId": "af4735e5-9772-48f1-916f-51ae1fd05e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'PSG_Predicting_Algorithm_Tags_and_Difficulty' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sronger/PSG_Predicting_Algorithm_Tags_and_Difficulty.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_H5kmPNuac4",
    "outputId": "90193373-cc1c-431d-9ec1-e5cbc3b86b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/klarakjomefischer/Downloads/Carleton/PSG_Predicting_Algorithm_Tags_and_Difficulty\n"
     ]
    }
   ],
   "source": [
    "%cd PSG_Predicting_Algorithm_Tags_and_Difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DD1vmaDKGc_k"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, RobertaTokenizer\n",
    "#from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import shutil\n",
    "from itertools import chain\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'PSG_Predicting_Algorithm_Tags_and_Difficulty', 'Copy_of_BigBird_based_Single_Task_Model.ipynb', '.venv', 'NLP', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "Qc7s4LGG4QCc",
    "outputId": "17289b72-ea59-4b49-e023-dedfc7594736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully!\n",
      "Train dataset head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>greedy</th>\n",
       "      <th>math</th>\n",
       "      <th>implementation</th>\n",
       "      <th>dp</th>\n",
       "      <th>constructive algorithms</th>\n",
       "      <th>data structures</th>\n",
       "      <th>brute force</th>\n",
       "      <th>binary search</th>\n",
       "      <th>sortings</th>\n",
       "      <th>graphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After years of hard work scientists invented a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traveling around the world you noticed that ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Burenka is the crown princess of Buryatia, and...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once Bob needed to find the second order stati...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petya and Gena love playing table tennis. A si...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement  greedy  math  \\\n",
       "0  After years of hard work scientists invented a...       1     0   \n",
       "1  Traveling around the world you noticed that ma...       0     0   \n",
       "2  Burenka is the crown princess of Buryatia, and...       1     1   \n",
       "3  Once Bob needed to find the second order stati...       0     0   \n",
       "4  Petya and Gena love playing table tennis. A si...       0     0   \n",
       "\n",
       "   implementation  dp  constructive algorithms  data structures  brute force  \\\n",
       "0               0   0                        1                0            0   \n",
       "1               0   1                        0                0            0   \n",
       "2               0   1                        0                0            0   \n",
       "3               0   0                        0                0            1   \n",
       "4               0   0                        0                0            0   \n",
       "\n",
       "   binary search  sortings  graphs  \n",
       "0              0         0       0  \n",
       "1              0         0       0  \n",
       "2              0         0       0  \n",
       "3              0         0       0  \n",
       "4              1         0       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>greedy</th>\n",
       "      <th>math</th>\n",
       "      <th>implementation</th>\n",
       "      <th>dp</th>\n",
       "      <th>constructive algorithms</th>\n",
       "      <th>data structures</th>\n",
       "      <th>brute force</th>\n",
       "      <th>binary search</th>\n",
       "      <th>sortings</th>\n",
       "      <th>graphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The black king is standing on a chess field co...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are given an integer x of n digits a_1, a_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are organizing a boxing tournament, where ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sherlock met Moriarty for a final battle of wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Of course you have heard the famous task about...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement  greedy  math  \\\n",
       "0  The black king is standing on a chess field co...       0     0   \n",
       "1  You are given an integer x of n digits a_1, a_...       1     0   \n",
       "2  You are organizing a boxing tournament, where ...       1     0   \n",
       "3  Sherlock met Moriarty for a final battle of wi...       0     0   \n",
       "4  Of course you have heard the famous task about...       1     0   \n",
       "\n",
       "   implementation  dp  constructive algorithms  data structures  brute force  \\\n",
       "0               0   0                        0                0            0   \n",
       "1               1   0                        1                0            0   \n",
       "2               0   1                        0                0            1   \n",
       "3               1   0                        1                1            0   \n",
       "4               0   1                        0                1            1   \n",
       "\n",
       "   binary search  sortings  graphs  \n",
       "0              0         0       1  \n",
       "1              0         0       0  \n",
       "2              0         0       0  \n",
       "3              0         0       1  \n",
       "4              0         1       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "import pandas as pd\n",
    "\n",
    "# Mount Google Drive\n",
    "#drive.mount('/content/drive')\n",
    "# Define the paths to the JSON files\n",
    "train_file_path = '../NLP/FinalProject/codeforces_train.jsonl'\n",
    "test_file_path = '../NLP/FinalProject/codeforces_test.jsonl'\n",
    "\n",
    "# Load the JSON files into pandas DataFrames\n",
    "try:\n",
    "    train_df = pd.read_json(train_file_path, lines=True)\n",
    "    test_df = pd.read_json(test_file_path, lines=True)\n",
    "    print(\"Datasets loaded successfully!\")\n",
    "    print(\"Train dataset head:\")\n",
    "    display(train_df.head()) # Display the first few rows of the train DataFrame\n",
    "    print(\"\\nTest dataset head:\")\n",
    "    display(test_df.head()) # Display the first few rows of the test DataFrame\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: One of the files was not found. Please check the paths.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the files: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "wQq7WvIHGc9r",
    "outputId": "c44b1f66-7de0-4eaf-8785-7cadc92b8d22"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1845/E</th>\n",
       "      <td>$$$ n $$$ box place a line box number $$$ 1 $$...</td>\n",
       "      <td>['dp', 'implementation', 'math']</td>\n",
       "      <td>2500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845/B</th>\n",
       "      <td>bob carol hang alice whole day 's time go home...</td>\n",
       "      <td>['geometry', 'implementation', 'math']</td>\n",
       "      <td>900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845/A</th>\n",
       "      <td>give integer $$$ n $$$ want obtain unlimited s...</td>\n",
       "      <td>['constructive algorithms', 'implementation', ...</td>\n",
       "      <td>800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841/B</th>\n",
       "      <td>array $$$ [ a_1 a_2 \\dots a_k ] $$$ call beaut...</td>\n",
       "      <td>['implementation']</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839/A</th>\n",
       "      <td>give two integers $$$ n $$$ $$$ k $$$ array $$...</td>\n",
       "      <td>['greedy', 'implementation', 'math']</td>\n",
       "      <td>800.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              description  \\\n",
       "1845/E  $$$ n $$$ box place a line box number $$$ 1 $$...   \n",
       "1845/B  bob carol hang alice whole day 's time go home...   \n",
       "1845/A  give integer $$$ n $$$ want obtain unlimited s...   \n",
       "1841/B  array $$$ [ a_1 a_2 \\dots a_k ] $$$ call beaut...   \n",
       "1839/A  give two integers $$$ n $$$ $$$ k $$$ array $$...   \n",
       "\n",
       "                                                     tags  rating  \n",
       "1845/E                   ['dp', 'implementation', 'math']  2500.0  \n",
       "1845/B             ['geometry', 'implementation', 'math']   900.0  \n",
       "1845/A  ['constructive algorithms', 'implementation', ...   800.0  \n",
       "1841/B                                 ['implementation']  1000.0  \n",
       "1839/A               ['greedy', 'implementation', 'math']   800.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_train_df = pd.read_csv('./data/AMT10/AMT10_train.csv', index_col=0, encoding='utf8')\n",
    "old_valid_df = pd.read_csv('./data/AMT10/AMT10_validation.csv', index_col=0, encoding='utf8')\n",
    "old_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "0be9402b",
    "outputId": "20dfb4b3-0ee9-4131-d3d3-6c2094dea063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New train_df head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After years of hard work scientists invented a...</td>\n",
       "      <td>[greedy, constructive algorithms]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traveling around the world you noticed that ma...</td>\n",
       "      <td>[dp]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Burenka is the crown princess of Buryatia, and...</td>\n",
       "      <td>[greedy, math, dp]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once Bob needed to find the second order stati...</td>\n",
       "      <td>[brute force]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petya and Gena love playing table tennis. A si...</td>\n",
       "      <td>[binary search]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  After years of hard work scientists invented a...   \n",
       "1  Traveling around the world you noticed that ma...   \n",
       "2  Burenka is the crown princess of Buryatia, and...   \n",
       "3  Once Bob needed to find the second order stati...   \n",
       "4  Petya and Gena love playing table tennis. A si...   \n",
       "\n",
       "                                tags  \n",
       "0  [greedy, constructive algorithms]  \n",
       "1                               [dp]  \n",
       "2                 [greedy, math, dp]  \n",
       "3                      [brute force]  \n",
       "4                    [binary search]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New test_df head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The black king is standing on a chess field co...</td>\n",
       "      <td>[graphs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are given an integer x of n digits a_1, a_...</td>\n",
       "      <td>[greedy, implementation, constructive algorithms]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are organizing a boxing tournament, where ...</td>\n",
       "      <td>[greedy, dp, brute force]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sherlock met Moriarty for a final battle of wi...</td>\n",
       "      <td>[implementation, constructive algorithms, data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Of course you have heard the famous task about...</td>\n",
       "      <td>[greedy, dp, data structures, brute force, sor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  The black king is standing on a chess field co...   \n",
       "1  You are given an integer x of n digits a_1, a_...   \n",
       "2  You are organizing a boxing tournament, where ...   \n",
       "3  Sherlock met Moriarty for a final battle of wi...   \n",
       "4  Of course you have heard the famous task about...   \n",
       "\n",
       "                                                tags  \n",
       "0                                           [graphs]  \n",
       "1  [greedy, implementation, constructive algorithms]  \n",
       "2                          [greedy, dp, brute force]  \n",
       "3  [implementation, constructive algorithms, data...  \n",
       "4  [greedy, dp, data structures, brute force, sor...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rename 'statement' to 'description' for consistency with old_train_df\n",
    "train_df = train_df.rename(columns={'statement': 'description'})\n",
    "test_df = test_df.rename(columns={'statement': 'description'})\n",
    "\n",
    "# Identify all boolean tag columns (all columns except 'description')\n",
    "jsonl_tag_columns = [col for col in train_df.columns if col != 'description']\n",
    "\n",
    "# Create a new 'tags' column as a list of strings\n",
    "# Each string is the name of a tag column where the value is 1\n",
    "train_df['tags'] = train_df.apply(lambda row: [col for col in jsonl_tag_columns if row[col] == 1], axis=1)\n",
    "test_df['tags'] = test_df.apply(lambda row: [col for col in jsonl_tag_columns if row[col] == 1], axis=1)\n",
    "\n",
    "# Drop the individual boolean tag columns as they are now consolidated into the 'tags' list\n",
    "train_df = train_df.drop(columns=jsonl_tag_columns)\n",
    "test_df = test_df.drop(columns=jsonl_tag_columns)\n",
    "\n",
    "# Note: Your JSONL data does not contain a 'rating' column, unlike 'old_train_df'.\n",
    "# If you intend to train for the 'rating' task, you will need to add a 'rating' column with actual data.\n",
    "\n",
    "print(\"New train_df head:\")\n",
    "display(train_df.head())\n",
    "print(\"\\nNew test_df head:\")\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "id": "924cd03f",
    "outputId": "394d7fd6-338c-4b7c-90ad-7c02ec2d4a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame Columns:\n",
      "Index(['description', 'tags'], dtype='object')\n",
      "\n",
      "Test DataFrame Columns:\n",
      "Index(['description', 'tags'], dtype='object')\n",
      "\n",
      "Train DataFrame Head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After years of hard work scientists invented a...</td>\n",
       "      <td>[greedy, constructive algorithms]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traveling around the world you noticed that ma...</td>\n",
       "      <td>[dp]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Burenka is the crown princess of Buryatia, and...</td>\n",
       "      <td>[greedy, math, dp]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once Bob needed to find the second order stati...</td>\n",
       "      <td>[brute force]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petya and Gena love playing table tennis. A si...</td>\n",
       "      <td>[binary search]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  After years of hard work scientists invented a...   \n",
       "1  Traveling around the world you noticed that ma...   \n",
       "2  Burenka is the crown princess of Buryatia, and...   \n",
       "3  Once Bob needed to find the second order stati...   \n",
       "4  Petya and Gena love playing table tennis. A si...   \n",
       "\n",
       "                                tags  \n",
       "0  [greedy, constructive algorithms]  \n",
       "1                               [dp]  \n",
       "2                 [greedy, math, dp]  \n",
       "3                      [brute force]  \n",
       "4                    [binary search]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test DataFrame Head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The black king is standing on a chess field co...</td>\n",
       "      <td>[graphs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are given an integer x of n digits a_1, a_...</td>\n",
       "      <td>[greedy, implementation, constructive algorithms]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are organizing a boxing tournament, where ...</td>\n",
       "      <td>[greedy, dp, brute force]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sherlock met Moriarty for a final battle of wi...</td>\n",
       "      <td>[implementation, constructive algorithms, data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Of course you have heard the famous task about...</td>\n",
       "      <td>[greedy, dp, data structures, brute force, sor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0  The black king is standing on a chess field co...   \n",
       "1  You are given an integer x of n digits a_1, a_...   \n",
       "2  You are organizing a boxing tournament, where ...   \n",
       "3  Sherlock met Moriarty for a final battle of wi...   \n",
       "4  Of course you have heard the famous task about...   \n",
       "\n",
       "                                                tags  \n",
       "0                                           [graphs]  \n",
       "1  [greedy, implementation, constructive algorithms]  \n",
       "2                          [greedy, dp, brute force]  \n",
       "3  [implementation, constructive algorithms, data...  \n",
       "4  [greedy, dp, data structures, brute force, sor...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Train DataFrame Columns:\")\n",
    "print(train_df.columns)\n",
    "print(\"\\nTest DataFrame Columns:\")\n",
    "print(test_df.columns)\n",
    "\n",
    "print(\"\\nTrain DataFrame Head:\")\n",
    "display(train_df.head())\n",
    "print(\"\\nTest DataFrame Head:\")\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oPQynsyYueoV"
   },
   "outputs": [],
   "source": [
    "AMT10 = [\n",
    "    'implementation',\n",
    "    'dp',\n",
    "    'math',\n",
    "    'greedy',\n",
    "    'data structures',\n",
    "    'brute force',\n",
    "    'geometry',\n",
    "    'constructive algorithms',\n",
    "    'dfs and similar',\n",
    "    'strings'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715,
     "referenced_widgets": [
      "484de4883dc24dab9963583886eeffdb",
      "db06551ae7514fffa77749c2ddf801aa",
      "cf7179344413494ea5ec843493afba37",
      "26cdd69505234db687ff96a5e8efd0b1",
      "44095dd5569a478aa0898bf4fd0b3cb4",
      "8f2bf9d95b6443bb9a8c28f9cc269b6a",
      "861e2a5d9db34a3295562191bee4c40c",
      "7a94033dd3164b2ca9e1632878c87b41",
      "26e421b1c7884c37b12b873b05092bbf",
      "4e19a277dc8048d595ca8aeb1900b71f",
      "04ee5024ed124c959a6c8dc752761f71"
     ]
    },
    "id": "7Gv2t8fcGc7r",
    "outputId": "539dd1d8-da12-47d9-a2f1-ac6c52f3c9be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigBirdConfig {\n",
       "  \"architectures\": [\n",
       "    \"BigBirdForPreTraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"attention_type\": \"block_sparse\",\n",
       "  \"block_size\": 64,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu_new\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"big_bird\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_random_blocks\": 3,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"rescale_embeddings\": false,\n",
       "  \"sep_token_id\": 66,\n",
       "  \"transformers_version\": \"4.57.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bias\": true,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50358\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained(\"google/bigbird-roberta-base\", max_position_embeddings=1024)\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "bdeea81b72d24a9b9567d6fc4662e09f",
      "27ccf80f36b6417fa1399309ed9b562b",
      "ed607d2e08b3425597a66a2d221b8074",
      "1367a1e6385d4767afb515deed361e0b",
      "2212779dac144a99930259343821a5a3",
      "1e1ea921d3b6498b9d75b405309cfc3d",
      "9c04e3fe5cac48969062307ef54fe3f4",
      "25c6f27c5c624a3081d3fa7a27535ca3",
      "76abadc8603a45faa488d763d73ebfaf",
      "f794d64e02c54feda1f9bd28219ca508",
      "0e3e8897c5f54e63b8a248ac6576e9d3",
      "9ba499dd8ded406a9bda8c992de9a797",
      "b203b22851e44cffa1e94da3747d76ea",
      "747b36208fee4b4d99896176df341c28",
      "011961397d0741e69835865111c0a4ae",
      "6790ba4dac9d406bbb18bdf253f35885",
      "6a914766add54e14b2549fa839819817",
      "6fbbadbf61124f89b6483ba7c4d76378",
      "8de8f6cbc38b481585ddd37627b265fa",
      "a987ebda5a9f4fcdaecfc1c4d4902ca1",
      "cd8f81b50f4446c69532b22446497e6b",
      "2433b712302140cb869846a2947e3e2e",
      "cb66f4aa56844a78b4f544916943f68a",
      "9d825aa518e742febc57cff4f37df153",
      "b91349a2cf27439fb9f00c3c0463ce25",
      "9ced980c19aa42a18575ac44c321ef43",
      "176d54957f27450882a2a7e70426070d",
      "74739f3e802d452fb9b5dd5e67434c38",
      "6cdd4191b1a1487daa6e179bce85bbfb",
      "923231b17ee24b3780dfbf4c1d859bcf",
      "fd4992d185944b1a8a1518d774c96141",
      "444a638775fe4ba7a2326110ba690352",
      "b4a463426aa146b0be8e408ca14dfc70",
      "35a05bf710424198bbcd635941985ff6",
      "36d4e579507f487b99c4befd94b1d3b5",
      "0d04b2e804ed4ab2b76840b303d9a8c6",
      "ef9b49a9cc944fe290c57c2a94bf3229",
      "b53c2b648aec4ae5a7d8f67adc0ee005",
      "fe528938babb4643b9f96fb5c6610065",
      "5901d8a0449c4396875badfbe84619ca",
      "39b72c77364b4189a72b557c7f5bbd1e",
      "dec1c3f2b6dc4eb8b35ea8028ef7e9d5",
      "757d270f52dd41e68b78b42053579f4e",
      "6839c11703e34093bc2b92ca42a7cd80",
      "76af5c97ae0b478eaf111d7fe5104ec6",
      "b445cb7c106d4565bbc717621e4404a7",
      "4cf4d47bef9b43bf911087a3de289f84",
      "6496eaa79bf2458cb26e506d445d6607",
      "dcd9c9c18fe1411fa3f8aed0580f1d4f",
      "05c4031d23b94df2955ffa4a1512dee2",
      "03a99e4433f44657ace9a4fda7cb3d0c",
      "2f78af593a0e43c6aef8e36d376ef827",
      "0ff73c0fd4a142b78c822217e1981d6b",
      "857592c7beaf4c799c2399445b98e19f",
      "7127004e57ec430d96ef6ff4cac7d994"
     ]
    },
    "id": "7jXGJfogGczD",
    "outputId": "2866f8f9-feae-48ea-c0ac-70a9be7b0d83"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'seed' : 42,\n",
    "    'tags' : AMT10,\n",
    "    'batchSize' : 4,\n",
    "    'lr' : 5e-6,\n",
    "    'trainMaxLength' : 1024,\n",
    "    'testMaxLength' : 1024,\n",
    "    'numEpochs' : 50,\n",
    "    'model' : AutoModel.from_config(model_config),\n",
    "    'tokenizer' : RobertaTokenizer.from_pretrained('roberta-base'),\n",
    "    'gradient_accumulation_steps' : 4,\n",
    "    'max_grad_norm' : 1.0,\n",
    "    'task' : 'tag', # rating, tag\n",
    "    'lambda' : 10,\n",
    "    'save' : True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "uPe4SOKmGcw6"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rdu_ph1GGcus",
    "outputId": "9c223500-fb0f-416c-bb6b-b41d2d97a912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7385\n"
     ]
    }
   ],
   "source": [
    "new_train_idx = []  # List to store new indices\n",
    "selected_train_tags = []  # List to store selected tags\n",
    "\n",
    "# Iterate through the DataFrame indices\n",
    "for index in train_df.index:\n",
    "    check = 0\n",
    "    t = []  # List to store selected tags for this index\n",
    "\n",
    "    # Iterate through the tags for the current index\n",
    "    for tag in train_df.loc[index]['tags']: # Removed ast.literal_eval\n",
    "        if tag in config['tags']:\n",
    "            check = 1\n",
    "            t.append(tag)\n",
    "\n",
    "    # If at least one tag is in the desired tags list, append the index and selected tags\n",
    "    if check == 1:\n",
    "        selected_train_tags.append(t)\n",
    "        new_train_idx.append(index)\n",
    "\n",
    "print(len(new_train_idx))  # Print the length of the new index list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXfztLXQuo0z",
    "outputId": "38608935-cc95-41a0-bfc0-425d7ddaa573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813\n"
     ]
    }
   ],
   "source": [
    "new_test_idx = []  # List to store new indices (changed from valid_idx)\n",
    "selected_test_tags = []  # List to store selected tags (changed from valid_tags)\n",
    "\n",
    "# Iterate through the DataFrame indices\n",
    "for index in test_df.index: # Changed from valid_df.index\n",
    "    check = 0\n",
    "    t = []  # List to store selected tags for this index\n",
    "\n",
    "    # Iterate through the tags for the current index\n",
    "    for tag in test_df.loc[index]['tags']: # Changed from valid_df.loc and removed ast.literal_eval\n",
    "        if tag in config['tags']:\n",
    "            check = 1\n",
    "            t.append(tag)\n",
    "\n",
    "    # If at least one tag is in the desired tags list, append the index and selected tags\n",
    "    if check == 1:\n",
    "        selected_test_tags.append(t)\n",
    "        new_test_idx.append(index)\n",
    "\n",
    "print(len(new_test_idx))  # Print the length of the new index list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VZBEzsKRuqS4"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.loc[new_train_idx]\n",
    "train_df['tags'] = selected_train_tags\n",
    "\n",
    "test_df = test_df.loc[new_test_idx] # Changed from valid_df\n",
    "test_df['tags'] = selected_test_tags # Changed from selected_valid_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ZK1NGl5OKfUk"
   },
   "outputs": [],
   "source": [
    "train_df['rating'] = np.zeros(len(train_df)).astype(int)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "# Using a fixed random_state for reproducibility\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=config['seed'])\n",
    "\n",
    "X_train = train_data['description']\n",
    "y_tags_train = train_data['tags']\n",
    "y_ratings_train = train_data['rating']\n",
    "\n",
    "X_val = val_data['description'] # This will be the new validation set\n",
    "y_tags_val = val_data['tags']\n",
    "y_ratings_val = val_data['rating']\n",
    "\n",
    "# X_test and related y_tags_test, y_ratings_test will remain but won't be used in Trainer\n",
    "# as the user requested to use validation data instead of testing data in the training.\n",
    "X_test_final = test_df['description'] # Renaming to avoid confusion with validation\n",
    "y_tags_test_final = test_df['tags']\n",
    "y_ratings_test_final = np.zeros(len(test_df)).astype(int) # Dummy ratings for final test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tags in TRAIN: 7\n",
      "Top TRAIN tag counts: [('greedy', 2137), ('math', 2093), ('implementation', 1770), ('dp', 1482), ('constructive algorithms', 1242), ('data structures', 1231), ('brute force', 1228)]\n",
      "\n",
      "Unique tags in VAL: 7\n",
      "Top VAL tag counts: [('greedy', 526), ('math', 503), ('implementation', 461), ('dp', 395), ('data structures', 320), ('constructive algorithms', 309), ('brute force', 295)]\n",
      "\n",
      "Unique tags in TEST (final): 7\n",
      "Top TEST tag counts: [('greedy', 291), ('implementation', 270), ('math', 265), ('dp', 212), ('constructive algorithms', 178), ('data structures', 173), ('brute force', 148)]\n",
      "\n",
      "AMT10 (normalized): ['implementation', 'dp', 'math', 'greedy', 'data structures', 'brute force', 'geometry', 'constructive algorithms', 'dfs and similar', 'strings']\n",
      "AMT10 missing from TRAIN: ['geometry', 'dfs and similar', 'strings']\n",
      "\n",
      "Sample train tags rows (first 10):\n",
      "0 ['math', 'dp']\n",
      "1 ['implementation', 'dp']\n",
      "2 ['greedy']\n",
      "3 ['greedy']\n",
      "4 ['math', 'brute force']\n",
      "5 ['greedy', 'math', 'constructive algorithms', 'brute force']\n",
      "6 ['greedy', 'constructive algorithms']\n",
      "7 ['greedy', 'dp']\n",
      "8 ['greedy', 'constructive algorithms']\n",
      "9 ['implementation', 'brute force']\n"
     ]
    }
   ],
   "source": [
    "# Robust diagnostic: print which AMT10 tags are present/missing and counts\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def extract_tag_list(value):\n",
    "    # Turn a single cell value into a list of tag strings (safe)\n",
    "    if isinstance(value, (list, tuple, set)):\n",
    "        return list(value)\n",
    "    if isinstance(value, str):\n",
    "        v = value.strip()\n",
    "        # try parse stringified list like \"['dp','math']\"\n",
    "        if v.startswith('[') and v.endswith(']'):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(v)\n",
    "                if isinstance(parsed, (list, tuple, set)):\n",
    "                    return list(parsed)\n",
    "            except Exception:\n",
    "                pass\n",
    "        # otherwise treat the string as a single tag or comma-separated list\n",
    "        if ',' in v and not v.startswith('['):\n",
    "            return [p.strip() for p in v.split(',') if p.strip()]\n",
    "        return [v]\n",
    "    # fallback: coerce to string\n",
    "    return [str(value)]\n",
    "\n",
    "def normalize_list(series):\n",
    "    out = []\n",
    "    for val in series:\n",
    "        tags = extract_tag_list(val)\n",
    "        for t in tags:\n",
    "            if t is None:\n",
    "                continue\n",
    "            out.append(str(t).strip().lower())\n",
    "    return out\n",
    "\n",
    "# Ensure the splits exist; prefer the explicit train_data if available\n",
    "if 'train_data' in globals():\n",
    "    train_series = train_data['tags']\n",
    "elif 'train_df' in globals():\n",
    "    train_series = train_df['tags']\n",
    "else:\n",
    "    raise NameError('train_df or train_data not defined. Run cells up to the train/test split first.')\n",
    "\n",
    "val_series = val_data['tags'] if 'val_data' in globals() else pd.Series([], dtype=object)\n",
    "test_series = test_df['tags'] if 'test_df' in globals() else pd.Series([], dtype=object)\n",
    "\n",
    "train_tags_norm = normalize_list(train_series)\n",
    "val_tags_norm = normalize_list(val_series)\n",
    "test_tags_norm = normalize_list(test_series)\n",
    "\n",
    "print('Unique tags in TRAIN:', len(set(train_tags_norm)))\n",
    "print('Top TRAIN tag counts:', Counter(train_tags_norm).most_common(50))\n",
    "print()\n",
    "print('Unique tags in VAL:', len(set(val_tags_norm)))\n",
    "print('Top VAL tag counts:', Counter(val_tags_norm).most_common(50))\n",
    "print()\n",
    "print('Unique tags in TEST (final):', len(set(test_tags_norm)))\n",
    "print('Top TEST tag counts:', Counter(test_tags_norm).most_common(50))\n",
    "print()\n",
    "AMT10_norm = [t.strip().lower() for t in AMT10] if 'AMT10' in globals() else []\n",
    "missing = [t for t in AMT10_norm if t not in set(train_tags_norm)]\n",
    "print('AMT10 (normalized):', AMT10_norm)\n",
    "print('AMT10 missing from TRAIN:', missing)\n",
    "print()\n",
    "print('Sample train tags rows (first 10):')\n",
    "sample_src = train_data['tags'] if 'train_data' in globals() else train_df['tags']\n",
    "for i, row in enumerate(sample_src.head(10)):\n",
    "    print(i, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kf1HtDq_Gcpz"
   },
   "outputs": [],
   "source": [
    "# Create an instance of the MultiLabelBinarizer\n",
    "tag_label_encoder = MultiLabelBinarizer()\n",
    "rating_label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the label encoder on the training labels and transform them\n",
    "y_tags_train_encoded = tag_label_encoder.fit_transform(y_tags_train)\n",
    "y_tags_val_encoded = tag_label_encoder.transform(y_tags_val) # Transform validation set\n",
    "y_tags_test_final_encoded = tag_label_encoder.transform(y_tags_test_final) # Transform final test set\n",
    "\n",
    "y_ratings_train_encoded = rating_label_encoder.fit_transform(y_ratings_train)\n",
    "y_ratings_val_encoded = rating_label_encoder.transform(y_ratings_val) # Transform validation set\n",
    "y_ratings_test_final_encoded = rating_label_encoder.transform(y_ratings_test_final) # Transform final test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YL7tlJ1eGcly"
   },
   "outputs": [],
   "source": [
    "# Define a class for multi-label classification head\n",
    "class MultiLabelClassificationHead(nn.Module):\n",
    "    def __init__(self, num_labels, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size) # Fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) # Fully connected layer\n",
    "        self.fc3 = nn.Linear(hidden_size, num_labels) # Fully connected layer\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x) # Apply the fully connected layer\n",
    "        x = self.fc2(x) # Apply the fully connected layer\n",
    "        x = self.fc3(x) # Apply the fully connected layer\n",
    "        x = self.sigmoid(x) # Apply the sigmoid activation\n",
    "        return x\n",
    "\n",
    "# Define a class for multi-class classification head\n",
    "class MultiClassClassificationHead(nn.Module):\n",
    "    def __init__(self, num_labels, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(hidden_size, num_labels)  # Fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)  # Apply the fully connected layer\n",
    "        return x\n",
    "\n",
    "# Define a classifier class\n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, model, device, tags_num_classes, ratings_num_classes):\n",
    "        super().__init__()\n",
    "        self.tags_num_classes = tags_num_classes  # Number of classes for tags\n",
    "        self.ratings_num_classes = ratings_num_classes  # Number of classes for ratings\n",
    "\n",
    "        # Set the device (GPU or CPU)\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize multi-label and multi-class classifiers\n",
    "        self.tags_classifier = MultiLabelClassificationHead(num_labels=self.tags_num_classes).to(self.device)\n",
    "        self.ratings_classifier = MultiClassClassificationHead(num_labels=self.ratings_num_classes).to(self.device)\n",
    "\n",
    "        # Define loss functions for multi-label and multi-class classification\n",
    "        self.BCE = nn.BCELoss().to(self.device)  # Binary Cross Entropy loss for multi-label classification\n",
    "        self.CE = nn.CrossEntropyLoss().to(self.device)  # Cross Entropy loss for multi-class classification\n",
    "\n",
    "        self.model = model\n",
    "        self.lr = config['lr']\n",
    "        self.task = config['task']\n",
    "\n",
    "        if self.task == 'tag':\n",
    "            self.parameters = [\n",
    "                    {'params': self.model.parameters()},\n",
    "                    {'params': self.tags_classifier.parameters()},\n",
    "                ]\n",
    "        elif self.task == 'rating':\n",
    "            self.parameters = [\n",
    "                    {'params': self.model.parameters()},\n",
    "                    {'params': self.ratings_classifier.parameters()}\n",
    "                ]\n",
    "\n",
    "        # Initialize the optimizer\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters,\n",
    "            lr=self.lr\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, tags_labels, ratings_labels):\n",
    "\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output # Pooled output from the model\n",
    "\n",
    "        if self.task == 'tag':\n",
    "          output = self.tags_classifier(pooled_output) # Predict tags using the tags classifier\n",
    "          loss = self.BCE(output, tags_labels) # Calculate the loss for tags\n",
    "        elif self.task == 'rating':\n",
    "          output = self.ratings_classifier(pooled_output) # Predict ratings using the ratings classifier\n",
    "          loss = self.CE(output, ratings_labels) # Calculate the loss for ratings\n",
    "\n",
    "        return loss, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOXiQD3wGcjz"
   },
   "outputs": [],
   "source": [
    "def tokenizing(tokenizer, data, max_length):\n",
    "    # Tokenize and encode the text input\n",
    "    data = list(data.values)\n",
    "    tokenized_data = tokenizer(data, padding=True, truncation=True, return_tensors='pt', max_length=max_length)\n",
    "\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CguouZ5RGchr"
   },
   "outputs": [],
   "source": [
    "def convert_to_tensor(data, dtype):\n",
    "    # Convert data to tensors\n",
    "    tensor_data = torch.tensor(data, dtype=dtype)\n",
    "    return tensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onmwykhkGcfq"
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 tokenized_inputs_train,\n",
    "                 tokenized_inputs_val,\n",
    "                 tags_labels_train,\n",
    "                 tags_labels_val,\n",
    "                 ratings_labels_train,\n",
    "                 ratings_labels_val\n",
    "                ):\n",
    "        # Set the device (GPU or CPU)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Store the input data and labels\n",
    "        self.tokenized_inputs_train = tokenized_inputs_train\n",
    "        self.tokenized_inputs_val = tokenized_inputs_val\n",
    "\n",
    "        self.tags_labels_train = tags_labels_train\n",
    "        self.tags_labels_val = tags_labels_val\n",
    "\n",
    "        self.ratings_labels_train = ratings_labels_train\n",
    "        self.ratings_labels_val = ratings_labels_val\n",
    "\n",
    "        # Determine the number of classes for tags and ratings\n",
    "        self.tags_num_classes = len(tags_labels_train[0])\n",
    "        self.ratings_num_classes = len(np.unique(ratings_labels_train))\n",
    "\n",
    "        # Move the model to the specified device\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        # Define Classifier Instance\n",
    "        self.classifier_instance = classifier(self.model, self.device, self.tags_num_classes, self.ratings_num_classes)\n",
    "\n",
    "        # Retrieve configuration parameters\n",
    "        self.batch_size = config['batchSize']\n",
    "        self.num_epochs = config['numEpochs']\n",
    "\n",
    "        self.accumulation_steps = config['gradient_accumulation_steps']\n",
    "        self.max_grad_norm = config['max_grad_norm']\n",
    "\n",
    "        self.tag_classes = tag_label_encoder.classes_\n",
    "\n",
    "        self.save = config['save']\n",
    "\n",
    "        # Initialize input data variables\n",
    "        self.input_ids_train = self.tokenized_inputs_train['input_ids']\n",
    "        self.attention_mask_train = self.tokenized_inputs_train['attention_mask']\n",
    "\n",
    "        self.input_ids_val = tokenized_inputs_val['input_ids']\n",
    "        self.attention_mask_val = tokenized_inputs_val['attention_mask']\n",
    "\n",
    "        self.task = config['task']\n",
    "\n",
    "    def train(self):\n",
    "        input_ids_train = self.input_ids_train\n",
    "        attention_mask_train = self.attention_mask_train\n",
    "        tags_labels_train = self.tags_labels_train\n",
    "        ratings_labels_train = self.ratings_labels_train\n",
    "\n",
    "        input_ids_val = self.input_ids_val\n",
    "        attention_mask_val = self.attention_mask_val\n",
    "        tags_labels_val = self.tags_labels_val\n",
    "        ratings_labels_val = self.ratings_labels_val\n",
    "\n",
    "        # Set the optimizer and learning rate\n",
    "        optimizer = self.classifier_instance.optimizer\n",
    "        parameters = self.classifier_instance.parameters\n",
    "\n",
    "        # Set the batch size\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        # Create a DataLoader for batching the data\n",
    "        train_dataset = TensorDataset(input_ids_train, attention_mask_train, tags_labels_train, ratings_labels_train)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "        valid_dataset = TensorDataset(input_ids_val, attention_mask_val, tags_labels_val, ratings_labels_val) # Using validation data\n",
    "        valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "        # Set the number of training epochs#\n",
    "        num_epochs = self.num_epochs\n",
    "        device = self.device\n",
    "\n",
    "        model = self.model\n",
    "        classifier_instance = self.classifier_instance\n",
    "\n",
    "        # Training loop\n",
    "        min_loss = 999999\n",
    "        rating_f1_s = 0\n",
    "        total_f1_s = 0\n",
    "        count = 0\n",
    "\n",
    "        #epochs\n",
    "\n",
    "        max_total_f1_macro_score_epochs = 0\n",
    "\n",
    "        max_tag_acc_epochs = 0\n",
    "        max_tag_f1_macro_epochs = 0\n",
    "        max_tag_f1_micro_epochs = 0\n",
    "        max_tag_f1_weighted_epochs= 0\n",
    "        max_tag_f1_samples_epochs= 0\n",
    "        max_tag_roc_auc_score_epochs = 0\n",
    "\n",
    "        max_rating_acc_epochs = 0\n",
    "        max_rating_f1_macro = 0\n",
    "        max_rating_f1_micro = 0\n",
    "        max_rating_f1_weighted = 0\n",
    "\n",
    "        #score\n",
    "        max_total_f1_macro_score = 0\n",
    "\n",
    "        max_tag_acc = 0\n",
    "        max_tag_f1_macro = 0\n",
    "        max_tag_f1_micro = 0\n",
    "        max_tag_f1_weighted = 0\n",
    "        max_tag_f1_samples = 0\n",
    "        max_tag_roc_auc_score = 0\n",
    "\n",
    "        max_rating_acc = 0\n",
    "        max_rating_f1_macro = 0\n",
    "        max_rating_f1_micro = 0\n",
    "        max_rating_f1_weighted = 0\n",
    "\n",
    "        thresholds = [0.001] + [i * 0.01 for i in range(1, 101)]\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # set early stopping\n",
    "            #if count > 8:\n",
    "            #    break\n",
    "            train_loss = 0.0\n",
    "            valid_loss = 0.0\n",
    "\n",
    "            tags_true = []\n",
    "            tags_pred = defaultdict(list)\n",
    "            tags_pred_proba = []\n",
    "\n",
    "            ratings_true = []\n",
    "            ratings_pred = []\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            # Training\n",
    "            classifier_instance.train()\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            for batch in tqdm(train_dataloader):\n",
    "                # Unpack the batch\n",
    "                input_ids, attention_mask, tags_labels, ratings_labels = batch\n",
    "\n",
    "                # Move the inputs and labels to the chosen device\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                tags_labels = tags_labels.to(device)\n",
    "                ratings_labels = ratings_labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                loss, _ = classifier_instance(input_ids, attention_mask, tags_labels, ratings_labels)\n",
    "                loss /= self.accumulation_steps\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "\n",
    "                if epoch % self.accumulation_steps ==  0 or epoch == batch_size - 1 or self.accumulation_steps == 0:\n",
    "                    if self.max_grad_norm > 0:\n",
    "\n",
    "                        if self.task == 'tag':\n",
    "                            torch.nn.utils.clip_grad_norm_(chain(\n",
    "                                model.parameters(),\n",
    "                                classifier_instance.tags_classifier.parameters()\n",
    "                            ), self.max_grad_norm)\n",
    "                        elif self.task == 'rating':\n",
    "                            torch.nn.utils.clip_grad_norm_(chain(\n",
    "                                model.parameters(),\n",
    "                                classifier_instance.ratings_classifier.parameters()\n",
    "                            ), self.max_grad_norm)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Validation\n",
    "            #model.eval()\n",
    "            classifier_instance.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(valid_dataloader):\n",
    "                    # Unpack the batch\n",
    "                    input_ids, attention_mask, tags_labels, ratings_labels = batch\n",
    "\n",
    "                    # Move the inputs and labels to the chosen device\n",
    "                    input_ids = input_ids.to(device)\n",
    "                    attention_mask = attention_mask.to(device)\n",
    "                    tags_labels = tags_labels.to(device)\n",
    "                    ratings_labels = ratings_labels.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    loss, output = classifier_instance(input_ids, attention_mask, tags_labels, ratings_labels)\n",
    "\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "                    if self.task == 'tag':\n",
    "                        tags_output = output\n",
    "\n",
    "                        # tags\n",
    "                        tags_pred_proba.extend(tags_output.detach().cpu().clone().tolist())\n",
    "\n",
    "                        # Extract indices where the value is above the threshold.\n",
    "                        for threshold in thresholds:\n",
    "                            tags_pred[threshold].extend([(row >= threshold).nonzero().flatten().tolist() for row in tags_output.detach().cpu().clone()])\n",
    "\n",
    "                        tags_true.extend([torch.nonzero(row).flatten().tolist() for row in tags_labels.detach().cpu().clone()])\n",
    "                    elif self.task == 'rating':\n",
    "\n",
    "                        ratings_output = output\n",
    "\n",
    "                        ratings_pred.extend(torch.argmax(ratings_output, dim=1).detach().cpu().clone())\n",
    "                        ratings_true.extend(ratings_labels.detach().cpu().clone())\n",
    "\n",
    "\n",
    "            # Calculate average loss\n",
    "            train_loss /= len(train_dataset)\n",
    "            valid_loss /= len(valid_dataset)\n",
    "\n",
    "\n",
    "            if epoch % self.accumulation_steps ==  0 or epoch == batch_size - 1 or self.accumulation_steps == 0:\n",
    "\n",
    "                # Print the loss, F1 score, precision, and recall for monitoring\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "                tag_true = []\n",
    "                #tag_pred = []\n",
    "\n",
    "                if self.task == 'rating':\n",
    "\n",
    "                    rating_true = [tensor.detach().cpu().clone().item() for tensor in ratings_true]\n",
    "                    rating_pred = [tensor.detach().cpu().clone().item() for tensor in ratings_pred]\n",
    "\n",
    "                    rating_k = defaultdict(list)\n",
    "\n",
    "                    for k in [0, 1, 2]:\n",
    "                        revise_rating_pred = []\n",
    "\n",
    "                        for i in range(len(rating_pred)):\n",
    "                            if abs(rating_true[i] - rating_pred[i]) <= k:\n",
    "                                revise_rating_pred.append(rating_true[i])\n",
    "                            else:\n",
    "                                revise_rating_pred.append(rating_pred[i])\n",
    "                        rating_k[k] = revise_rating_pred\n",
    "\n",
    "                    rating_pred = rating_k[1]\n",
    "\n",
    "                    rat_t = Counter(rating_true)\n",
    "                    rat_p = Counter(rating_pred)\n",
    "\n",
    "                    epoch_max_rating_acc = 0\n",
    "                    epoch_max_rating_f1_macro = 0\n",
    "                    epoch_max_rating_f1_micro = 0\n",
    "                    epoch_max_rating_f1_weighted = 0\n",
    "\n",
    "                    # rating\n",
    "\n",
    "                    for k in [0, 1, 2]:\n",
    "                        rating_pred = rating_k[k]\n",
    "\n",
    "                        rating_acc = accuracy_score(rating_true, rating_pred)\n",
    "                        rating_f1_macro = f1_score(rating_true, rating_pred, average='macro', zero_division=0)\n",
    "                        rating_f1_micro = f1_score(rating_true, rating_pred, average='micro', zero_division=0)\n",
    "                        rating_f1_weighted = f1_score(rating_true, rating_pred, average='weighted', zero_division=0)\n",
    "\n",
    "                        if k == 1:\n",
    "                            epoch_max_rating_acc = max(epoch_max_rating_acc, rating_acc)\n",
    "                            epoch_max_rating_f1_macro = max(epoch_max_rating_f1_macro, rating_f1_macro)\n",
    "                            epoch_max_rating_f1_micro = max(epoch_max_rating_f1_micro, rating_f1_micro)\n",
    "                            epoch_max_rating_f1_weighted = max(epoch_max_rating_f1_weighted, rating_f1_weighted)\n",
    "\n",
    "                        #rating\n",
    "                        print(f\"rating acc Max Score in this epoch at {k}:\", rating_acc)\n",
    "                        print(f\"rating valid Max F1 Score(macro) per class in this epoch at {k}:\", rating_f1_macro)\n",
    "                        print(f\"rating valid Max F1 Score(micro) per class in this epoch at {k}:\", rating_f1_micro)\n",
    "                        print(f\"rating valid Max F1 Score(weighted) per class in this epoch at {k}:\", rating_f1_weighted)\n",
    "                        print()\n",
    "\n",
    "                    #rating\n",
    "                    print(\"rating acc Max Score in this epoch:\", epoch_max_rating_acc)\n",
    "                    print(\"rating valid Max F1 Score(macro) per class in this epoch:\", epoch_max_rating_f1_macro)\n",
    "                    print(\"rating valid Max F1 Score(micro) per class in this epoch:\", epoch_max_rating_f1_micro)\n",
    "                    print(\"rating valid Max F1 Score(weighted) per class in this epoch:\", epoch_max_rating_f1_weighted)\n",
    "                    print()\n",
    "                    print('rating_true : ', sorted(rat_t.items(), key=lambda x: x[0]))\n",
    "                    print('rating_pred : ', sorted(rat_p.items(), key=lambda x: x[0]))\n",
    "                    print()\n",
    "\n",
    "\n",
    "                    #rating\n",
    "                    print(f\"rating acc Max Score: {max_rating_acc} at {max_rating_acc_epochs}epochs\")\n",
    "                    print(f\"rating valid Max F1 Score(macro) per class: {max_rating_f1_macro} at {max_rating_f1_macro_epochs}epochs\")\n",
    "                    print(f\"rating valid Max F1 Score(micro) per class: {max_rating_f1_micro} at {max_rating_f1_micro_epochs}epochs\")\n",
    "                    print(f\"rating valid Max F1 Score(weighted) per class: {max_rating_f1_weighted} at {max_rating_f1_weighted_epochs}epochs\")\n",
    "                    print()\n",
    "\n",
    "                    # rating\n",
    "\n",
    "                    if epoch_max_rating_acc > max_rating_acc:\n",
    "                        max_rating_acc_epochs = epoch\n",
    "                        max_rating_acc = max(epoch_max_rating_acc, max_rating_acc)\n",
    "\n",
    "                    if epoch_max_rating_f1_macro > max_rating_f1_macro:\n",
    "                        max_rating_f1_macro_epochs = epoch\n",
    "                        max_rating_f1_macro = max(epoch_max_rating_f1_macro, max_rating_f1_macro)\n",
    "\n",
    "                        now = datetime.now()\n",
    "                        task = config['task']\n",
    "                        if self.save:\n",
    "                            self.save_checkpoint(task, model, epoch)\n",
    "                        count = 0\n",
    "                        print('Best Model Saved !')\n",
    "                        print()\n",
    "\n",
    "                    if epoch_max_rating_f1_micro > max_rating_f1_micro:\n",
    "                        max_rating_f1_micro_epochs = epoch\n",
    "                        max_rating_f1_micro = max(epoch_max_rating_f1_micro, max_rating_f1_micro)\n",
    "\n",
    "                    if epoch_max_rating_f1_weighted > max_rating_f1_weighted:\n",
    "                        max_rating_f1_weighted_epochs = epoch\n",
    "                        max_rating_f1_weighted = max(epoch_max_rating_f1_weighted, max_rating_f1_weighted)\n",
    "\n",
    "                elif self.task == 'tag':\n",
    "\n",
    "                    for index_list in tags_true:\n",
    "                        result_true = [0] * self.tags_num_classes  # Create a list of length num_classes.\n",
    "                        for index in index_list:\n",
    "                            result_true[index] = 1  # Fill the corresponding index with 1.\n",
    "\n",
    "                        tag_true.append(result_true)\n",
    "\n",
    "\n",
    "                    epoch_max_tag_acc = 0\n",
    "                    epoch_max_tag_f1_macro = 0\n",
    "                    epoch_max_tag_f1_micro = 0\n",
    "                    epoch_max_tag_f1_weighted = 0\n",
    "                    epoch_max_tag_f1_samples = 0\n",
    "\n",
    "                    epoch_max_tag_roc_auc_score = roc_auc_score(tag_true, tags_pred_proba)\n",
    "                    tag_true = np.array(tag_true)\n",
    "                    tags_pred_proba = np.array(tags_pred_proba)\n",
    "\n",
    "                    for threshold in thresholds:\n",
    "                        tag_pred = []\n",
    "                        for index_list in tags_pred[threshold]:\n",
    "                            result_pred = [0] * self.tags_num_classes  # Create a list of length num_classes.\n",
    "                            for index in index_list:\n",
    "                                result_pred[index] = 1 # Fill the corresponding index with 1.\n",
    "\n",
    "                            tag_pred.append(result_pred)\n",
    "\n",
    "                        # tag\n",
    "\n",
    "                        tag_acc = accuracy_score(tag_true, tag_pred)\n",
    "                        tag_f1_macro = f1_score(tag_true, tag_pred, average='macro', zero_division=0)\n",
    "                        tag_f1_micro = f1_score(tag_true, tag_pred, average='micro', zero_division=0)\n",
    "                        tag_f1_weighted = f1_score(tag_true, tag_pred, average='weighted', zero_division=0)\n",
    "                        tag_f1_samples = f1_score(tag_true, tag_pred, average='samples', zero_division=0)\n",
    "\n",
    "                        epoch_max_tag_acc = max(epoch_max_tag_acc, tag_acc)\n",
    "                        epoch_max_tag_f1_macro = max(epoch_max_tag_f1_macro, tag_f1_macro)\n",
    "                        epoch_max_tag_f1_micro = max(epoch_max_tag_f1_micro, tag_f1_micro)\n",
    "                        epoch_max_tag_f1_weighted = max(epoch_max_tag_f1_weighted, tag_f1_weighted)\n",
    "                        epoch_max_tag_f1_samples = max(epoch_max_tag_f1_samples, tag_f1_samples)\n",
    "\n",
    "                    #tag\n",
    "                    print(\"tag acc Max Score in this epoch:\", epoch_max_tag_acc)\n",
    "                    print(\"tag valid Max F1 Score(macro) per class in this epoch:\", epoch_max_tag_f1_macro)\n",
    "                    print(\"tag valid Max F1 Score(micro) per class in this epoch:\", epoch_max_tag_f1_micro)\n",
    "                    print(\"tag valid Max F1 Score(weighted) per class in this epoch:\", epoch_max_tag_f1_weighted)\n",
    "                    print(\"tag valid Max F1 Score(samples) per class in this epoch:\", epoch_max_tag_f1_samples)\n",
    "                    print()\n",
    "                    print(\"tag valid Max roc_auc_score avg in this epoch:\", epoch_max_tag_roc_auc_score)\n",
    "\n",
    "                    for num_classes in range(self.tags_num_classes):\n",
    "                        score = roc_auc_score(tag_true[:, num_classes], tags_pred_proba[:, num_classes])\n",
    "                        print(f\"{self.tag_classes[num_classes]} : {score}\")\n",
    "                    print()\n",
    "\n",
    "                    #tag\n",
    "                    print(f\"tag acc Max Score: {max_tag_acc} at {max_tag_acc_epochs}epochs\")\n",
    "                    print(f\"tag valid Max F1 Score(macro) per class: {max_tag_f1_macro} at {max_tag_f1_macro_epochs}epochs\")\n",
    "                    print(f\"tag valid Max F1 Score(micro) per class: {max_tag_f1_micro} at {max_tag_f1_micro_epochs}epochs\")\n",
    "                    print(f\"tag valid Max F1 Score(weighted) per class: {max_tag_f1_weighted} at {max_tag_f1_weighted_epochs}epochs\")\n",
    "                    print(f\"tag valid Max F1 Score(samples) per class: {max_tag_f1_samples} at {max_tag_f1_samples_epochs}epochs\")\n",
    "                    print(f\"tag valid Max roc_auc_score: {max_tag_roc_auc_score} at {max_tag_roc_auc_score_epochs}epochs\")\n",
    "                    print()\n",
    "\n",
    "                    # tag\n",
    "\n",
    "                    if epoch_max_tag_acc > max_tag_acc:\n",
    "                        max_tag_acc_epochs = epoch\n",
    "                        max_tag_acc = max(epoch_max_tag_acc, max_tag_acc)\n",
    "\n",
    "                    if epoch_max_tag_f1_macro > max_tag_f1_macro:\n",
    "                        max_tag_f1_macro_epochs = epoch\n",
    "                        max_tag_f1_macro = max(epoch_max_tag_f1_macro, max_tag_f1_macro)\n",
    "\n",
    "                        now = datetime.now()\n",
    "                        task = config['task']\n",
    "                        if self.save:\n",
    "                            self.save_checkpoint(task, model, epoch)\n",
    "                        count = 0\n",
    "                        print('Best Model Saved !')\n",
    "                        print()\n",
    "\n",
    "                    if epoch_max_tag_f1_micro > max_tag_f1_micro:\n",
    "                        max_tag_f1_micro_epochs = epoch\n",
    "                        max_tag_f1_micro = max(epoch_max_tag_f1_micro, max_tag_f1_micro)\n",
    "\n",
    "                    if epoch_max_tag_f1_weighted > max_tag_f1_weighted:\n",
    "                        max_tag_f1_weighted_epochs = epoch\n",
    "                        max_tag_f1_weighted = max(epoch_max_tag_f1_weighted, max_tag_f1_weighted)\n",
    "\n",
    "                    if epoch_max_tag_f1_samples > max_tag_f1_samples:\n",
    "                        max_tag_f1_samples_epochs = epoch\n",
    "                        max_tag_f1_samples = max(epoch_max_tag_f1_samples, max_tag_f1_samples)\n",
    "\n",
    "                    if epoch_max_tag_roc_auc_score > max_tag_roc_auc_score:\n",
    "                        max_tag_roc_auc_score_epochs = epoch\n",
    "                        max_tag_roc_auc_score = max(epoch_max_tag_roc_auc_score, max_tag_roc_auc_score)\n",
    "\n",
    "                print('----------------------------------------------------------------------------')\n",
    "                print()\n",
    "\n",
    "    def save_checkpoint(self, task, model, epoch, max_checkpoints=5):\n",
    "        now = datetime.now()\n",
    "        today = now.strftime('%Y-%m-%d')\n",
    "        checkpoint_filename = f\"{now.strftime('%Y-%m-%d')}_{epoch + 1}\"\n",
    "        checkpoint_path = os.path.join(f\"./models/{task}/{today}\", checkpoint_filename)\n",
    "\n",
    "        # If the directory does not exist, create it.\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            os.makedirs(checkpoint_path)\n",
    "\n",
    "        # Save the model state_dict\n",
    "        torch.save(self.classifier_instance.state_dict(), os.path.join(checkpoint_path, f\"model.pt\"))\n",
    "        checkpoint_files = sorted(os.listdir(f\"./models/{task}/{today}\"))\n",
    "        # Delete oldest checkpoint if there are too many\n",
    "        while len(checkpoint_files) > max_checkpoints + 1:\n",
    "            checkpoint_files = sorted(os.listdir(f\"./models/{task}/{today}\"))\n",
    "            oldest_checkpoint = os.path.join(f\"./models/{task}/{today}\", checkpoint_files[0])\n",
    "            #os.remove(oldest_checkpoint)\n",
    "            if os.path.exists(oldest_checkpoint) and os.path.isdir(oldest_checkpoint):\n",
    "                # Check if the directory is empty, and if not, use shutil.rmtree() to recursively delete it.\n",
    "                try:\n",
    "                    shutil.rmtree(oldest_checkpoint)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while deleting directory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYL4JnAxGcdb"
   },
   "outputs": [],
   "source": [
    "tokenizer = config['tokenizer']\n",
    "model = config['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ppu5jEkjGcbj"
   },
   "outputs": [],
   "source": [
    "tokenized_inputs_train = tokenizing(tokenizer, X_train, config['trainMaxLength'])\n",
    "tokenized_inputs_val = tokenizing(tokenizer, X_val, config['testMaxLength']) # New validation tokenization\n",
    "tokenized_inputs_test_final = tokenizing(tokenizer, X_test_final, config['testMaxLength']) # Final test tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HaNnefJtGcZa"
   },
   "outputs": [],
   "source": [
    "tags_labels_train = convert_to_tensor(y_tags_train_encoded, dtype=torch.float)\n",
    "tags_labels_val = convert_to_tensor(y_tags_val_encoded, dtype=torch.float) # New validation tensor\n",
    "ratings_labels_train = convert_to_tensor(y_ratings_train_encoded, dtype=torch.long)\n",
    "ratings_labels_val = convert_to_tensor(y_ratings_val_encoded, dtype=torch.long) # New validation tensor\n",
    "\n",
    "# Keep test_final for later, but not pass to Trainer for validation\n",
    "tags_labels_test_final = convert_to_tensor(y_tags_test_final_encoded, dtype=torch.float)\n",
    "ratings_labels_test_final = convert_to_tensor(y_ratings_test_final_encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9T-aDDKjGcXc"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model,\n",
    "                 tokenized_inputs_train,\n",
    "                 tokenized_inputs_val, # Use validation set here\n",
    "                 tags_labels_train,\n",
    "                 tags_labels_val, # Use validation set here\n",
    "                 ratings_labels_train,\n",
    "                 ratings_labels_val # Use validation set here\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHU5bFOBGcVL",
    "outputId": "2cfe3422-f1d1-407a-cdb9-6052fb2c9e2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1477 [00:00<?, ?it/s]/Users/klarakjomefischer/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Input ids are automatically padded from 1011 to 1024 to be a multiple of `config.block_size`: 64\n",
      " 12%|        | 174/1477 [22:15<2:55:45,  8.09s/it]libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x10d705ee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/klarakjomefischer/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/klarakjomefischer/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1618, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/connection.py\", line 1136, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/selectors.py\", line 425, in select\n",
      "    key = self._key_from_fd(fd)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/selectors.py\", line 275, in _key_from_fd\n",
      "    def _key_from_fd(self, fd):\n",
      "\n",
      "  File \"/Users/klarakjomefischer/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py\", line 73, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 93890) is killed by signal: Abort trap: 6. \n",
      " 12%|        | 174/1477 [22:19<2:47:12,  7.70s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 156\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    153\u001b[39m ratings_labels = ratings_labels.to(device)\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m loss, _ = \u001b[43mclassifier_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratings_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m loss /= \u001b[38;5;28mself\u001b[39m.accumulation_steps\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mclassifier.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, tags_labels, ratings_labels)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, tags_labels, ratings_labels):\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     pooled_output = outputs.pooler_output \u001b[38;5;66;03m# Pooled output from the model\u001b[39;00m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.task == \u001b[33m'\u001b[39m\u001b[33mtag\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/transformers/models/big_bird/modeling_big_bird.py:1989\u001b[39m, in \u001b[36mBigBirdModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[39m\n\u001b[32m   1979\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m   1981\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m   1982\u001b[39m     input_ids=input_ids,\n\u001b[32m   1983\u001b[39m     position_ids=position_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1986\u001b[39m     past_key_values_length=past_key_values_length,\n\u001b[32m   1987\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1989\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1990\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1991\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1992\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1994\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1996\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1997\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1998\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mband_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mband_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblocked_encoder_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocked_encoder_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2005\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2006\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   2008\u001b[39m pooler_output = \u001b[38;5;28mself\u001b[39m.activation(\u001b[38;5;28mself\u001b[39m.pooler(sequence_output[:, \u001b[32m0\u001b[39m, :])) \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/transformers/models/big_bird/modeling_big_bird.py:1586\u001b[39m, in \u001b[36mBigBirdEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, band_mask, from_mask, to_mask, blocked_encoder_mask, return_dict, cache_position)\u001b[39m\n\u001b[32m   1582\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m   1584\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1586\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mband_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mto_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblocked_encoder_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1597\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1598\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1599\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1601\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1602\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/transformers/models/big_bird/modeling_big_bird.py:1506\u001b[39m, in \u001b[36mBigBirdLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, band_mask, from_mask, to_mask, blocked_encoder_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m   1503\u001b[39m     attention_output = cross_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1504\u001b[39m     outputs = outputs + cross_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1506\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1507\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (layer_output,) + outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/transformers/pytorch_utils.py:257\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/transformers/models/big_bird/modeling_big_bird.py:1514\u001b[39m, in \u001b[36mBigBirdLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m   1512\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m   1513\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m-> \u001b[39m\u001b[32m1514\u001b[39m     layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1515\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/transformers/models/big_bird/modeling_big_bird.py:1416\u001b[39m, in \u001b[36mBigBirdOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m   1414\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m   1415\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dense(hidden_states)\n\u001b[32m-> \u001b[39m\u001b[32m1416\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1417\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n\u001b[32m   1418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m     70\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Carleton/.venv/lib/python3.12/site-packages/torch/nn/functional.py:1418\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1416\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1417\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1418\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1419\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkrYhqpMv2bO"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HDvJZO5GcTC"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9iQ3T9uGcQq"
   },
   "outputs": [],
   "source": [
    "state = torch.load('./model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9a71eeQGcOq"
   },
   "outputs": [],
   "source": [
    "model_state_dict = {}\n",
    "tag_state_dict = {}\n",
    "rating_state_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEZTgWXLGcMk"
   },
   "outputs": [],
   "source": [
    "for k, v in state.items():\n",
    "    if \"model.\" in k:\n",
    "        name = k[6:]\n",
    "        model_state_dict[name] = v\n",
    "    if \"tags_classifier.\" in k:\n",
    "        name = k[len(\"tags_classifier.\"):]\n",
    "        tag_state_dict[name] = v\n",
    "    if \"ratings_classifier.\" in k:\n",
    "        name = k[len(\"ratings_classifier.\"):]\n",
    "        rating_state_dict[name] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_DSrqetGcKb"
   },
   "outputs": [],
   "source": [
    "model = config['model']\n",
    "tag_head = MultiLabelClassificationHead(10)\n",
    "rating_head = MultiClassClassificationHead(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMzigxAdGcIc"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(model_state_dict)\n",
    "tag_head.load_state_dict(tag_state_dict)\n",
    "rating_head.load_state_dict(rating_state_dict)\n",
    "print('fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlLSK8NBK6cO"
   },
   "outputs": [],
   "source": [
    "# Set the device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the chosen device\n",
    "model.to(device)\n",
    "tag_head.to(device)\n",
    "rating_head.to(device)\n",
    "print('device : ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AH2-JiW0K6am"
   },
   "outputs": [],
   "source": [
    "valid_dataset = TensorDataset(tokenized_inputs_test_final['input_ids'], tokenized_inputs_test_final['attention_mask'], tags_labels_test_final, ratings_labels_test_final)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=config['batchSize'], shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9txbqiWeK6Y2"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "tag_head.eval()\n",
    "rating_head.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    thresholds = [0.001] + [i * 0.01 for i in range(1, 101)]\n",
    "    tags_true = []\n",
    "    tags_pred = defaultdict(list)\n",
    "    tags_pred_proba = []\n",
    "\n",
    "    ratings_true = []\n",
    "    ratings_pred = []\n",
    "    for batch in tqdm(valid_dataloader):\n",
    "        ## Unpack the batch\n",
    "        input_ids, attention_mask, tags_labels, ratings_labels = batch\n",
    "\n",
    "        # Move the inputs and labels to the chosen device\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        tags_labels = tags_labels.to(device)\n",
    "        ratings_labels = ratings_labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        tags_output = tag_head(pooled_output)\n",
    "        ratings_output = rating_head(pooled_output)\n",
    "\n",
    "        # tags\n",
    "        tags_true.extend([torch.nonzero(row).flatten().tolist() for row in tags_labels.detach().cpu().clone()])\n",
    "        tags_pred_proba.extend(tags_output.detach().cpu().clone().tolist())\n",
    "\n",
    "        ratings_pred.extend(torch.argmax(ratings_output, dim=1).detach().cpu().clone())\n",
    "        ratings_true.extend(ratings_labels.detach().cpu().clone())\n",
    "\n",
    "        # Extract indices with values greater than or equal to the threshold.\n",
    "        for threshold in thresholds:\n",
    "            tags_pred[threshold].extend([(row >= threshold).nonzero().flatten().tolist() for row in tags_output.detach().cpu().clone()])\n",
    "\n",
    "    rating_true = [tensor.detach().cpu().clone().item() for tensor in ratings_true]\n",
    "    rating_pred = [tensor.detach().cpu().clone().item() for tensor in ratings_pred]\n",
    "\n",
    "    revise_rating_pred = []\n",
    "\n",
    "    for i in range(len(rating_pred)):\n",
    "        if abs(rating_true[i] - rating_pred[i]) <= 1:\n",
    "            revise_rating_pred.append(rating_true[i])\n",
    "        else:\n",
    "            revise_rating_pred.append(rating_pred[i])\n",
    "\n",
    "    rating_pred = revise_rating_pred\n",
    "\n",
    "    tag_true = []\n",
    "\n",
    "    for index_list in tags_true:\n",
    "        result_true = [0] * 10  # Create a list of length num_classes.\n",
    "        for index in index_list:\n",
    "            result_true[index] = 1  # Fill the corresponding index with 1.\n",
    "\n",
    "        tag_true.append(result_true)\n",
    "\n",
    "    tag_true = np.array(tag_true)\n",
    "    tags_pred_proba = np.array(tags_pred_proba)\n",
    "\n",
    "    thr = 0\n",
    "    max_f1_score = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        tag_pred = []\n",
    "        for index_list in tags_pred[threshold]:\n",
    "            result_pred = [0] * 10 # Create a list of length num_classes.\n",
    "            for index in index_list:\n",
    "                result_pred[index] = 1  # Fill the corresponding index with 1.\n",
    "\n",
    "            tag_pred.append(result_pred)\n",
    "\n",
    "        f1 = f1_score(tag_true, tag_pred, average='macro', zero_division=0)\n",
    "        if max_f1_score < f1:\n",
    "            thr = threshold\n",
    "            max_f1_score = f1\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "\n",
    "    # Plot ROC curve for each classifier\n",
    "    plt.figure()\n",
    "    for num_classes in range(10):\n",
    "        tt, tp = tag_true[:, num_classes], tags_pred_proba[:, num_classes]\n",
    "\n",
    "        score = roc_auc_score(tt, tp)\n",
    "        tag = tag_label_encoder.classes_[num_classes]\n",
    "        print(f\"{tag} : {score}\")\n",
    "        fpr[num_classes], tpr[num_classes], _ = roc_curve(tt, tp)\n",
    "        plt.plot(fpr[num_classes], tpr[num_classes], label=f'{tag}(area={score:.2f})')\n",
    "    print()\n",
    "\n",
    "    print(\"tag_roc_auc_score : \", roc_auc_score(tag_true, tags_pred_proba))\n",
    "    print(\"f1_score : \", max_f1_score)\n",
    "    print(\"threshold : \", thr)\n",
    "\n",
    "    rating_acc = accuracy_score(rating_true, rating_pred)\n",
    "    print(f\"rating_acc : {rating_acc}\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Tag Prediction')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEDoyOhuK6XG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjYo0oVKK6VP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSQXQuaCK6TX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "011961397d0741e69835865111c0a4ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd8f81b50f4446c69532b22446497e6b",
      "placeholder": "",
      "style": "IPY_MODEL_2433b712302140cb869846a2947e3e2e",
      "value": "899k/899k[00:00&lt;00:00,7.94MB/s]"
     }
    },
    "03a99e4433f44657ace9a4fda7cb3d0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "04ee5024ed124c959a6c8dc752761f71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "05c4031d23b94df2955ffa4a1512dee2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d04b2e804ed4ab2b76840b303d9a8c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39b72c77364b4189a72b557c7f5bbd1e",
      "max": 1355863,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dec1c3f2b6dc4eb8b35ea8028ef7e9d5",
      "value": 1355863
     }
    },
    "0e3e8897c5f54e63b8a248ac6576e9d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0ff73c0fd4a142b78c822217e1981d6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1367a1e6385d4767afb515deed361e0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f794d64e02c54feda1f9bd28219ca508",
      "placeholder": "",
      "style": "IPY_MODEL_0e3e8897c5f54e63b8a248ac6576e9d3",
      "value": "25.0/25.0[00:00&lt;00:00,2.54kB/s]"
     }
    },
    "176d54957f27450882a2a7e70426070d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e1ea921d3b6498b9d75b405309cfc3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2212779dac144a99930259343821a5a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2433b712302140cb869846a2947e3e2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "25c6f27c5c624a3081d3fa7a27535ca3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26cdd69505234db687ff96a5e8efd0b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e19a277dc8048d595ca8aeb1900b71f",
      "placeholder": "",
      "style": "IPY_MODEL_04ee5024ed124c959a6c8dc752761f71",
      "value": "760/760[00:00&lt;00:00,82.8kB/s]"
     }
    },
    "26e421b1c7884c37b12b873b05092bbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "27ccf80f36b6417fa1399309ed9b562b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e1ea921d3b6498b9d75b405309cfc3d",
      "placeholder": "",
      "style": "IPY_MODEL_9c04e3fe5cac48969062307ef54fe3f4",
      "value": "tokenizer_config.json:100%"
     }
    },
    "2f78af593a0e43c6aef8e36d376ef827": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35a05bf710424198bbcd635941985ff6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_36d4e579507f487b99c4befd94b1d3b5",
       "IPY_MODEL_0d04b2e804ed4ab2b76840b303d9a8c6",
       "IPY_MODEL_ef9b49a9cc944fe290c57c2a94bf3229"
      ],
      "layout": "IPY_MODEL_b53c2b648aec4ae5a7d8f67adc0ee005"
     }
    },
    "36d4e579507f487b99c4befd94b1d3b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe528938babb4643b9f96fb5c6610065",
      "placeholder": "",
      "style": "IPY_MODEL_5901d8a0449c4396875badfbe84619ca",
      "value": "tokenizer.json:100%"
     }
    },
    "39b72c77364b4189a72b557c7f5bbd1e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44095dd5569a478aa0898bf4fd0b3cb4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "444a638775fe4ba7a2326110ba690352": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "484de4883dc24dab9963583886eeffdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_db06551ae7514fffa77749c2ddf801aa",
       "IPY_MODEL_cf7179344413494ea5ec843493afba37",
       "IPY_MODEL_26cdd69505234db687ff96a5e8efd0b1"
      ],
      "layout": "IPY_MODEL_44095dd5569a478aa0898bf4fd0b3cb4"
     }
    },
    "4cf4d47bef9b43bf911087a3de289f84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f78af593a0e43c6aef8e36d376ef827",
      "max": 481,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0ff73c0fd4a142b78c822217e1981d6b",
      "value": 481
     }
    },
    "4e19a277dc8048d595ca8aeb1900b71f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5901d8a0449c4396875badfbe84619ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6496eaa79bf2458cb26e506d445d6607": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_857592c7beaf4c799c2399445b98e19f",
      "placeholder": "",
      "style": "IPY_MODEL_7127004e57ec430d96ef6ff4cac7d994",
      "value": "481/481[00:00&lt;00:00,66.1kB/s]"
     }
    },
    "6790ba4dac9d406bbb18bdf253f35885": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6839c11703e34093bc2b92ca42a7cd80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6a914766add54e14b2549fa839819817": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6cdd4191b1a1487daa6e179bce85bbfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fbbadbf61124f89b6483ba7c4d76378": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7127004e57ec430d96ef6ff4cac7d994": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "74739f3e802d452fb9b5dd5e67434c38": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "747b36208fee4b4d99896176df341c28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8de8f6cbc38b481585ddd37627b265fa",
      "max": 898823,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a987ebda5a9f4fcdaecfc1c4d4902ca1",
      "value": 898823
     }
    },
    "757d270f52dd41e68b78b42053579f4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76abadc8603a45faa488d763d73ebfaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "76af5c97ae0b478eaf111d7fe5104ec6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b445cb7c106d4565bbc717621e4404a7",
       "IPY_MODEL_4cf4d47bef9b43bf911087a3de289f84",
       "IPY_MODEL_6496eaa79bf2458cb26e506d445d6607"
      ],
      "layout": "IPY_MODEL_dcd9c9c18fe1411fa3f8aed0580f1d4f"
     }
    },
    "7a94033dd3164b2ca9e1632878c87b41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "857592c7beaf4c799c2399445b98e19f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "861e2a5d9db34a3295562191bee4c40c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8de8f6cbc38b481585ddd37627b265fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f2bf9d95b6443bb9a8c28f9cc269b6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "923231b17ee24b3780dfbf4c1d859bcf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ba499dd8ded406a9bda8c992de9a797": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b203b22851e44cffa1e94da3747d76ea",
       "IPY_MODEL_747b36208fee4b4d99896176df341c28",
       "IPY_MODEL_011961397d0741e69835865111c0a4ae"
      ],
      "layout": "IPY_MODEL_6790ba4dac9d406bbb18bdf253f35885"
     }
    },
    "9c04e3fe5cac48969062307ef54fe3f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ced980c19aa42a18575ac44c321ef43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_444a638775fe4ba7a2326110ba690352",
      "placeholder": "",
      "style": "IPY_MODEL_b4a463426aa146b0be8e408ca14dfc70",
      "value": "456k/456k[00:00&lt;00:00,37.1MB/s]"
     }
    },
    "9d825aa518e742febc57cff4f37df153": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74739f3e802d452fb9b5dd5e67434c38",
      "placeholder": "",
      "style": "IPY_MODEL_6cdd4191b1a1487daa6e179bce85bbfb",
      "value": "merges.txt:100%"
     }
    },
    "a987ebda5a9f4fcdaecfc1c4d4902ca1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b203b22851e44cffa1e94da3747d76ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a914766add54e14b2549fa839819817",
      "placeholder": "",
      "style": "IPY_MODEL_6fbbadbf61124f89b6483ba7c4d76378",
      "value": "vocab.json:100%"
     }
    },
    "b445cb7c106d4565bbc717621e4404a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05c4031d23b94df2955ffa4a1512dee2",
      "placeholder": "",
      "style": "IPY_MODEL_03a99e4433f44657ace9a4fda7cb3d0c",
      "value": "config.json:100%"
     }
    },
    "b4a463426aa146b0be8e408ca14dfc70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b53c2b648aec4ae5a7d8f67adc0ee005": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b91349a2cf27439fb9f00c3c0463ce25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_923231b17ee24b3780dfbf4c1d859bcf",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fd4992d185944b1a8a1518d774c96141",
      "value": 456318
     }
    },
    "bdeea81b72d24a9b9567d6fc4662e09f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_27ccf80f36b6417fa1399309ed9b562b",
       "IPY_MODEL_ed607d2e08b3425597a66a2d221b8074",
       "IPY_MODEL_1367a1e6385d4767afb515deed361e0b"
      ],
      "layout": "IPY_MODEL_2212779dac144a99930259343821a5a3"
     }
    },
    "cb66f4aa56844a78b4f544916943f68a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d825aa518e742febc57cff4f37df153",
       "IPY_MODEL_b91349a2cf27439fb9f00c3c0463ce25",
       "IPY_MODEL_9ced980c19aa42a18575ac44c321ef43"
      ],
      "layout": "IPY_MODEL_176d54957f27450882a2a7e70426070d"
     }
    },
    "cd8f81b50f4446c69532b22446497e6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf7179344413494ea5ec843493afba37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a94033dd3164b2ca9e1632878c87b41",
      "max": 760,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_26e421b1c7884c37b12b873b05092bbf",
      "value": 760
     }
    },
    "db06551ae7514fffa77749c2ddf801aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f2bf9d95b6443bb9a8c28f9cc269b6a",
      "placeholder": "",
      "style": "IPY_MODEL_861e2a5d9db34a3295562191bee4c40c",
      "value": "config.json:100%"
     }
    },
    "dcd9c9c18fe1411fa3f8aed0580f1d4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dec1c3f2b6dc4eb8b35ea8028ef7e9d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ed607d2e08b3425597a66a2d221b8074": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25c6f27c5c624a3081d3fa7a27535ca3",
      "max": 25,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76abadc8603a45faa488d763d73ebfaf",
      "value": 25
     }
    },
    "ef9b49a9cc944fe290c57c2a94bf3229": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_757d270f52dd41e68b78b42053579f4e",
      "placeholder": "",
      "style": "IPY_MODEL_6839c11703e34093bc2b92ca42a7cd80",
      "value": "1.36M/1.36M[00:00&lt;00:00,38.9MB/s]"
     }
    },
    "f794d64e02c54feda1f9bd28219ca508": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd4992d185944b1a8a1518d774c96141": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fe528938babb4643b9f96fb5c6610065": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
