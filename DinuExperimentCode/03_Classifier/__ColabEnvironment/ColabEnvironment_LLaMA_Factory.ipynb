{"cells":[{"cell_type":"markdown","id":"06dbf06c","metadata":{"id":"06dbf06c"},"source":["For google colab environment"]},{"cell_type":"code","execution_count":null,"id":"9027fe86","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2734,"status":"ok","timestamp":1749898146296,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-180},"id":"9027fe86","outputId":"dfb2adb0-b5f3-4cde-914e-b45dc61de45d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"uvPLs_teLmWe","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1749898146306,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-180},"id":"uvPLs_teLmWe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9afcb3bb-1eaa-441c-a5f2-7c2f66983319"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier/LLaMA-Factory\n"]}],"source":["cd \"/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier/LLaMA-Factory/\""]},{"cell_type":"code","execution_count":null,"id":"rGNSIXpTYhbR","metadata":{"executionInfo":{"elapsed":3616,"status":"ok","timestamp":1749898149923,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-180},"id":"rGNSIXpTYhbR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"de675a87-5460-46c5-feab-9a1ad90afcaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.12.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.4.26)\n"]}],"source":["pip install --upgrade huggingface_hub\n"]},{"cell_type":"code","execution_count":null,"id":"mw929vmxYdyw","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5021,"status":"ok","timestamp":1749898154946,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-180},"id":"mw929vmxYdyw","outputId":"bfff1dc5-84ba-4279-b5a4-8e499f7c6655"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) n\n","Token is valid (permission: fineGrained).\n","The token `LLM_TRAIN` has been saved to /root/.cache/huggingface/stored_tokens\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `LLM_TRAIN`\n"]}],"source":["!huggingface-cli login"]},{"cell_type":"code","execution_count":null,"id":"kHCJhs0kNfju","metadata":{"executionInfo":{"elapsed":15548,"status":"ok","timestamp":1749898172988,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-180},"id":"kHCJhs0kNfju","colab":{"base_uri":"https://localhost:8080/"},"outputId":"528cfe99-1700-48a2-8ac8-54b5da7c1acb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n","Obtaining file:///content/drive/Othercomputers/ASUS%20Main/MLCP/03_Classifier/LLaMA-Factory\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (4.51.3)\n","Requirement already satisfied: datasets<=3.5.0,>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.5.0)\n","Requirement already satisfied: accelerate<=1.6.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.6.0)\n","Requirement already satisfied: peft<=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.15.1)\n","Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.9.6)\n","Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.21.1)\n","Requirement already satisfied: gradio<=5.25.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.25.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.15.3)\n","Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.29.5)\n","Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.34.3)\n","Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.115.12)\n","Requirement already satisfied: sse-starlette in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.3.6)\n","Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.10.0)\n","Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.7.0)\n","Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n","Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\n","Requirement already satisfied: pydantic<=2.10.6 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.10.6)\n","Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n","Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (14.4.0)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.11.0)\n","Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.14)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (2.6.0+cu124)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.33.0)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.7)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.15)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2024.12.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.15)\n","Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (24.1.0)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n","Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.6.0)\n","Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.8.0)\n","Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n","Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.0.2)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.18)\n","Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.2.1)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.0.20)\n","Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.11.12)\n","Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.6)\n","Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.10.0)\n","Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.46.2)\n","Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.13.2)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.16.0)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.14.0)\n","Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (2.27.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (1.1.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->llamafactory==0.9.3.dev0) (2024.11.6)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (13.9.4)\n","Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n","Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (1.7.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.20.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.4.26)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.16.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.58.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->llamafactory==0.9.3.dev0) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.4.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.19.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (3.5)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (1.3.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.3.dev0) (3.1.0)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n","Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.6.1)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.5.1)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n","Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.8)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->llamafactory==0.9.3.dev0) (4.9.3)\n","Building wheels for collected packages: llamafactory\n","  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26743 sha256=1633d8787d961e0e69b2a4ea3fa5a0fec38922a85c9be854185804d7d7a6450b\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-f1oszegr/wheels/56/c9/c3/6f1bef4ffcb5ed14069ff7f945804c6a0c82d3e76e40c30f96\n","Successfully built llamafactory\n","Installing collected packages: llamafactory\n","  Attempting uninstall: llamafactory\n","    Found existing installation: llamafactory 0.9.3.dev0\n","    Uninstalling llamafactory-0.9.3.dev0:\n","      Successfully uninstalled llamafactory-0.9.3.dev0\n","Successfully installed llamafactory-0.9.3.dev0\n","Requirement already satisfied: rouge_chinese in /usr/local/lib/python3.11/dist-packages (1.0.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge_chinese) (1.17.0)\n"]}],"source":["!python -m pip install --upgrade pip\n","!pip install -e .\n","!pip install rouge_chinese"]},{"cell_type":"markdown","id":"9ea53468","metadata":{"id":"9ea53468"},"source":["Start code here"]},{"cell_type":"code","execution_count":null,"id":"t65lvNZUO9ad","metadata":{"id":"t65lvNZUO9ad"},"outputs":[],"source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","os.environ[\"GEMMA_ATTENTION_IMPL\"]=\"eager\"\n"]},{"cell_type":"code","source":["!python src/webui.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rUSxzr-2xvOY","outputId":"3b2771b6-aa2f-4df3-c0d8-f9357a8210cb","executionInfo":{"status":"ok","timestamp":1749907662652,"user_tz":-180,"elapsed":5158902,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"}}},"id":"rUSxzr-2xvOY","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-06-14 12:01:50.764593: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-06-14 12:01:50.782542: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1749902510.804409   21670 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1749902510.811087   21670 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-06-14 12:01:50.832882: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n","* Running on local URL:  http://0.0.0.0:7860\n","* Running on public URL: https://e3780f88b84b0e97c9.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","2025-06-14 12:04:23.097777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1749902663.119337   22374 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1749902663.125840   22374 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","[INFO|2025-06-14 12:04:30] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 12:04:30,448 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 12:04:30,449 >> loading file tokenizer.model from cache at None\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 12:04:30,449 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 12:04:30,449 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 12:04:30,449 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 12:04:30,449 >> loading file chat_template.jinja from cache at None\n","[INFO|tokenization_utils_base.py:2323] 2025-06-14 12:04:30,934 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|configuration_utils.py:693] 2025-06-14 12:04:31,324 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/config.json\n","[INFO|configuration_utils.py:765] 2025-06-14 12:04:31,326 >> Model config LlamaConfig {\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.51.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 12:04:31,396 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 12:04:31,396 >> loading file tokenizer.model from cache at None\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 12:04:31,396 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 12:04:31,396 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 12:04:31,396 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 12:04:31,396 >> loading file chat_template.jinja from cache at None\n","[INFO|tokenization_utils_base.py:2323] 2025-06-14 12:04:31,871 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|2025-06-14 12:04:31] llamafactory.data.template:143 >> Add pad token: <|end_of_text|>\n","[INFO|2025-06-14 12:04:31] llamafactory.data.loader:143 >> Loading dataset top_5_training_dataset.json...\n","Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n","WARNING:datasets.builder:Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n","Generating train split: 4459 examples [00:00, 8509.83 examples/s]\n","Converting format of dataset (num_proc=16): 100% 4459/4459 [00:00<00:00, 16826.39 examples/s]\n","[INFO|2025-06-14 12:04:34] llamafactory.data.loader:143 >> Loading dataset top_5_validation_dataset.json...\n","Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n","WARNING:datasets.builder:Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n","Generating train split: 482 examples [00:00, 3356.22 examples/s]\n","Converting format of dataset (num_proc=16): 100% 482/482 [00:00<00:00, 2267.67 examples/s]\n","Running tokenizer on dataset (num_proc=16): 100% 4941/4941 [00:05<00:00, 890.26 examples/s] \n","training example:\n","input_ids:\n","[39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 2675, 527, 264, 7452, 4877, 34465, 627, 5715, 27785, 279, 8581, 9681, 505, 420, 8521, 1160, 512, 681, 15893, 9497, 518, 364, 10590, 518, 364, 14706, 518, 364, 9996, 518, 364, 695, 14726, 4532, 5207, 7041, 279, 9681, 11, 43147, 11, 19180, 555, 32783, 29466, 29047, 13, 2360, 1023, 4339, 627, 4959, 1988, 17610, 315, 512, 482, 362, 15840, 10039, 10329, 5224, 627, 482, 1556, 76994, 66233, 12730, 1, 3857, 430, 15100, 279, 10825, 6425, 627, 10464, 86589, 5224, 11, 323, 28144, 311, 10491, 9681, 627, 14711, 67346, 220, 16, 720, 32298, 25, 16644, 459, 1358, 315, 308, 26864, 11, 4320, 2874, 2134, 8187, 20126, 627, 9597, 532, 25, 5075, 4734, 449, 264, 34544, 2007, 304, 507, 1471, 1515, 308, 705, 1243, 4320, 304, 507, 7, 16, 4390, 16533, 25, 828, 14726, 198, 14711, 67346, 220, 17, 720, 32298, 25, 1472, 617, 452, 19289, 323, 2218, 2694, 328, 13, 5560, 1855, 16652, 520, 1455, 3131, 627, 9597, 532, 25, 8012, 11581, 1004, 1483, 73, 60, 284, 69384, 37498, 1701, 1176, 602, 19289, 627, 16533, 25, 11581, 271, 7184, 49229, 512, 15000, 3418, 8035, 374, 459, 47302, 315, 264, 9084, 1974, 19845, 4977, 15481, 1567, 2684, 527, 308, 23978, 304, 9084, 1974, 49926, 505, 220, 16, 311, 308, 3735, 3418, 8035, 8964, 682, 15022, 55946, 304, 279, 5654, 2684, 527, 308, 4236, 279, 602, 270, 5575, 374, 37191, 520, 264, 12374, 577, 602, 323, 706, 264, 15840, 10151, 274, 602, 3735, 3418, 8035, 706, 311, 10491, 389, 279, 5718, 1457, 763, 4040, 279, 1396, 315, 3697, 304, 279, 2128, 3735, 3418, 8035, 8964, 430, 422, 568, 41011, 279, 1404, 315, 279, 2128, 311, 387, 1063, 7698, 597, 1855, 12374, 690, 3708, 872, 597, 31005, 449, 279, 8592, 15840, 10151, 274, 4236, 304, 279, 1176, 2128, 279, 1828, 597, 31005, 4236, 304, 279, 2132, 2128, 323, 779, 389, 1442, 1070, 527, 17162, 1109, 597, 4236, 2163, 1243, 279, 2128, 649, 259, 387, 14454, 7181, 430, 1070, 2643, 387, 23978, 430, 3708, 7315, 7411, 578, 8333, 315, 279, 5654, 374, 279, 2860, 10151, 315, 279, 3697, 315, 682, 3118, 7411, 1442, 1070, 527, 912, 7411, 3118, 1243, 279, 8333, 374, 220, 15, 11736, 3735, 3418, 8035, 311, 1505, 279, 8333, 315, 279, 5654, 369, 1855, 5873, 315, 597, 505, 220, 16, 311, 308, 198, 4521, 66233, 55023, 3947, 527, 1403, 3062, 24654, 311, 1304, 578, 1176, 832, 374, 430, 499, 649, 11294, 279, 11503, 369, 1855, 12374, 29235, 315, 1855, 1023, 323, 2694, 1124, 709, 311, 6994, 279, 837, 4320, 578, 2132, 832, 374, 430, 422, 1070, 527, 4236, 304, 459, 12374, 1243, 430, 12374, 649, 1193, 17210, 311, 11503, 369, 505, 311, 2100, 422, 584, 4048, 311, 11294, 279, 19035, 315, 279, 270, 12374, 369, 1063, 8521, 304, 1243, 584, 690, 387, 3025, 311, 30729, 927, 682, 3284, 369, 1855, 12374, 323, 636, 279, 6425, 304, 1405, 374, 279, 1396, 315, 4236, 304, 279, 270, 12374, 2057, 11322, 433, 499, 617, 311, 9762, 279, 2694, 315, 279, 7340, 1396, 315, 4236, 430, 649, 1376, 2539, 7411, 315, 1404, 3011, 2011, 387, 279, 8592, 1396, 2753, 1109, 477, 6273, 311, 430, 374, 76016, 555, 779, 16347, 279, 4236, 315, 1855, 12374, 864, 36697, 7276, 37498, 323, 1457, 499, 527, 1949, 311, 923, 279, 9436, 2694, 315, 430, 1396, 315, 4236, 311, 279, 4320, 369, 28993, 23965, 824, 71180, 271, 14711, 6075, 512, 695, 14726, 11, 57080, 128001, 271]\n","inputs:\n","Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","You are a strict tag classifier.\n","Return ONLY the applicable tags from this fixed list:\n","['greedy','math', 'implementation', 'dp', 'data structures']\n","Output exactly the tags, lowercase, separated by comma-no-space. No other words.\n","Each input consists of:\n"," - A programming-problem statement.\n"," - An \"--- Editorial ---\" section that explains the intended solution.\n","Use BOTH statement, and editorial to decide tags.\n","### EXAMPLE 1 \n","Problem: Given an array of n integers, answer q range minimum queries.\n","Editorial: Preprocess with a sparse table in O(n log n), then answer in O(1).\n","Answer: data structures\n","### EXAMPLE 2 \n","Problem: You have N coins and target sum S. Use each coin at most once.\n","Editorial: Build dp[i][j] = reachable sums using first i coins.\n","Answer: dp\n","\n","Now classify:\n","Polycarp is an organizer of a Berland ICPC regional event There are n universities in Berland numbered from 1 to n Polycarp knows all competitive programmers in the region There are n students the i th student is enrolled at a university u i and has a programming skill s i Polycarp has to decide on the rules now In particular the number of members in the team Polycarp knows that if he chooses the size of the team to be some integer k each university will send their k strongest with the highest programming skill s students in the first team the next k strongest students in the second team and so on If there are fewer than k students left then the team can t be formed Note that there might be universities that send zero teams The strength of the region is the total skill of the members of all present teams If there are no teams present then the strength is 0 Help Polycarp to find the strength of the region for each choice of k from 1 to n\n","--- Editorial ---\n","There are two important observations to make The first one is that you can calculate the answers for each university independently of each other and sum them up to obtain the true answer The second one is that if there are students in an university then that university can only contribute to answers for from to So if we learn to calculate the contribution of the th university for some fixed in then we will be able to iterate over all possible for each university and get the solution in where is the number of students in the th university To achieve it you have to gather the sum of the maximum number of students that can form full teams of size That must be the highest number less than or equal to that is divisible by so Sort the students of each university precalculate partial sums and now you are free to add the prefix sum of that number of students to the answer for Overall complexity per testcase\n","\n","### Response:\n","data structures, greedy<|end_of_text|>\n","\n","\n","label_ids:\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 695, 14726, 11, 57080, 128001, 271]\n","labels:\n","data structures, greedy<|end_of_text|>\n","\n","\n","[INFO|configuration_utils.py:693] 2025-06-14 12:04:42,201 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/config.json\n","[INFO|configuration_utils.py:765] 2025-06-14 12:04:42,202 >> Model config LlamaConfig {\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.51.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|2025-06-14 12:04:42] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n","[INFO|modeling_utils.py:1124] 2025-06-14 12:04:42,261 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/model.safetensors.index.json\n","[INFO|modeling_utils.py:2167] 2025-06-14 12:04:42,261 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n","[INFO|configuration_utils.py:1142] 2025-06-14 12:04:42,263 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"use_cache\": false\n","}\n","\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.19s/it]\n","[INFO|modeling_utils.py:4930] 2025-06-14 12:04:47,057 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","[INFO|modeling_utils.py:4938] 2025-06-14 12:04:47,058 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","[INFO|configuration_utils.py:1097] 2025-06-14 12:04:47,136 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/generation_config.json\n","[INFO|configuration_utils.py:1142] 2025-06-14 12:04:47,136 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": 128001,\n","  \"max_length\": 4096,\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","[INFO|2025-06-14 12:04:47] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n","[INFO|2025-06-14 12:04:47] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n","[INFO|2025-06-14 12:04:47] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n","[INFO|2025-06-14 12:04:47] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n","[INFO|2025-06-14 12:04:48] llamafactory.model.loader:143 >> trainable params: 83,886,080 || all params: 8,114,147,328 || trainable%: 1.0338\n","[INFO|trainer.py:748] 2025-06-14 12:04:48,413 >> Using auto half precision backend\n","[INFO|trainer.py:2414] 2025-06-14 12:04:48,905 >> ***** Running training *****\n","[INFO|trainer.py:2415] 2025-06-14 12:04:48,905 >>   Num examples = 4,941\n","[INFO|trainer.py:2416] 2025-06-14 12:04:48,905 >>   Num Epochs = 3\n","[INFO|trainer.py:2417] 2025-06-14 12:04:48,905 >>   Instantaneous batch size per device = 8\n","[INFO|trainer.py:2420] 2025-06-14 12:04:48,905 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n","[INFO|trainer.py:2421] 2025-06-14 12:04:48,905 >>   Gradient Accumulation steps = 4\n","[INFO|trainer.py:2422] 2025-06-14 12:04:48,906 >>   Total optimization steps = 462\n","[INFO|trainer.py:2423] 2025-06-14 12:04:48,910 >>   Number of trainable parameters = 83,886,080\n","  5% 25/462 [04:12<1:14:14, 10.19s/it][INFO|2025-06-14 12:09:00] llamafactory.train.callbacks:143 >> {'loss': 3.8733, 'learning_rate': 2.4000e-05, 'epoch': 0.16, 'throughput': 2785.11}\n","{'loss': 3.8733, 'grad_norm': 3.1856982707977295, 'learning_rate': 2.4e-05, 'epoch': 0.16, 'num_input_tokens_seen': 702080}\n"," 11% 50/462 [08:27<1:12:27, 10.55s/it][INFO|2025-06-14 12:13:16] llamafactory.train.callbacks:143 >> {'loss': 0.7390, 'learning_rate': 2.9857e-05, 'epoch': 0.32, 'throughput': 2793.36}\n","{'loss': 0.739, 'grad_norm': 2.6214210987091064, 'learning_rate': 2.985704160501954e-05, 'epoch': 0.32, 'num_input_tokens_seen': 1417472}\n"," 16% 75/462 [12:33<1:07:22, 10.45s/it][INFO|2025-06-14 12:17:22] llamafactory.train.callbacks:143 >> {'loss': 0.5563, 'learning_rate': 2.9239e-05, 'epoch': 0.49, 'throughput': 2795.30}\n","{'loss': 0.5563, 'grad_norm': 5.3909382820129395, 'learning_rate': 2.923863965359551e-05, 'epoch': 0.49, 'num_input_tokens_seen': 2106496}\n"," 22% 100/462 [16:44<59:01,  9.78s/it][INFO|2025-06-14 12:21:33] llamafactory.train.callbacks:143 >> {'loss': 0.4789, 'learning_rate': 2.8151e-05, 'epoch': 0.65, 'throughput': 2796.43}\n","{'loss': 0.4789, 'grad_norm': 2.2144975662231445, 'learning_rate': 2.815090133561262e-05, 'epoch': 0.65, 'num_input_tokens_seen': 2808768}\n"," 27% 125/462 [20:52<56:03,  9.98s/it][INFO|2025-06-14 12:25:41] llamafactory.train.callbacks:143 >> {'loss': 0.4724, 'learning_rate': 2.6630e-05, 'epoch': 0.81, 'throughput': 2796.31}\n","{'loss': 0.4724, 'grad_norm': 2.8621766567230225, 'learning_rate': 2.6629680859221987e-05, 'epoch': 0.81, 'num_input_tokens_seen': 3501568}\n"," 32% 150/462 [25:02<53:03, 10.20s/it][INFO|2025-06-14 12:29:51] llamafactory.train.callbacks:143 >> {'loss': 0.4687, 'learning_rate': 2.4725e-05, 'epoch': 0.97, 'throughput': 2796.53}\n","{'loss': 0.4687, 'grad_norm': 1.619882345199585, 'learning_rate': 2.472512094230214e-05, 'epoch': 0.97, 'num_input_tokens_seen': 4202304}\n"," 38% 175/462 [29:12<47:24,  9.91s/it][INFO|2025-06-14 12:34:01] llamafactory.train.callbacks:143 >> {'loss': 0.4334, 'learning_rate': 2.2500e-05, 'epoch': 1.14, 'throughput': 2796.84}\n","{'loss': 0.4334, 'grad_norm': 2.3548007011413574, 'learning_rate': 2.25e-05, 'epoch': 1.14, 'num_input_tokens_seen': 4902400}\n"," 43% 200/462 [33:18<44:30, 10.19s/it][INFO|2025-06-14 12:38:07] llamafactory.train.callbacks:143 >> {'loss': 0.3994, 'learning_rate': 2.0028e-05, 'epoch': 1.30, 'throughput': 2796.60}\n","{'loss': 0.3994, 'grad_norm': 3.0002262592315674, 'learning_rate': 2.0027662832354125e-05, 'epoch': 1.3, 'num_input_tokens_seen': 5588928}\n"," 43% 200/462 [33:18<44:30, 10.19s/it][INFO|trainer.py:3984] 2025-06-14 12:38:07,391 >> Saving model checkpoint to saves/Llama-3-8B/lora/train_2025-06-14-12-02-52_top_5/checkpoint-200\n","[INFO|configuration_utils.py:693] 2025-06-14 12:38:07,585 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/config.json\n","[INFO|configuration_utils.py:765] 2025-06-14 12:38:07,586 >> Model config LlamaConfig {\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.51.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2510] 2025-06-14 12:38:08,473 >> tokenizer config file saved in saves/Llama-3-8B/lora/train_2025-06-14-12-02-52_top_5/checkpoint-200/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2519] 2025-06-14 12:38:08,478 >> Special tokens file saved in saves/Llama-3-8B/lora/train_2025-06-14-12-02-52_top_5/checkpoint-200/special_tokens_map.json\n"," 49% 225/462 [37:36<40:42, 10.31s/it][INFO|2025-06-14 12:42:25] llamafactory.train.callbacks:143 >> {'loss': 0.4059, 'learning_rate': 1.7390e-05, 'epoch': 1.46, 'throughput': 2789.95}\n","{'loss': 0.4059, 'grad_norm': 2.085651159286499, 'learning_rate': 1.738960302101284e-05, 'epoch': 1.46, 'num_input_tokens_seen': 6296768}\n"," 54% 250/462 [41:51<35:29, 10.04s/it][INFO|2025-06-14 12:46:40] llamafactory.train.callbacks:143 >> {'loss': 0.4121, 'learning_rate': 1.4673e-05, 'epoch': 1.62, 'throughput': 2790.70}\n","{'loss': 0.4121, 'grad_norm': 2.358689785003662, 'learning_rate': 1.4672776724481583e-05, 'epoch': 1.62, 'num_input_tokens_seen': 7008384}\n"," 60% 275/462 [46:00<30:00,  9.63s/it][INFO|2025-06-14 12:50:49] llamafactory.train.callbacks:143 >> {'loss': 0.3988, 'learning_rate': 1.1967e-05, 'epoch': 1.78, 'throughput': 2791.48}\n","{'loss': 0.3988, 'grad_norm': 1.552001953125, 'learning_rate': 1.1966736415019434e-05, 'epoch': 1.78, 'num_input_tokens_seen': 7707264}\n"," 65% 300/462 [50:17<28:23, 10.51s/it][INFO|2025-06-14 12:55:06] llamafactory.train.callbacks:143 >> {'loss': 0.3983, 'learning_rate': 9.3607e-06, 'epoch': 1.94, 'throughput': 2791.92}\n","{'loss': 0.3983, 'grad_norm': 2.178410530090332, 'learning_rate': 9.360679035417479e-06, 'epoch': 1.94, 'num_input_tokens_seen': 8424256}\n"," 70% 325/462 [54:31<22:17,  9.76s/it][INFO|2025-06-14 12:59:20] llamafactory.train.callbacks:143 >> {'loss': 0.3539, 'learning_rate': 6.9405e-06, 'epoch': 2.11, 'throughput': 2792.52}\n","{'loss': 0.3539, 'grad_norm': 1.857788324356079, 'learning_rate': 6.9405058747976406e-06, 'epoch': 2.11, 'num_input_tokens_seen': 9136192}\n"," 76% 350/462 [58:39<18:44, 10.04s/it][INFO|2025-06-14 13:03:27] llamafactory.train.callbacks:143 >> {'loss': 0.3431, 'learning_rate': 4.7860e-06, 'epoch': 2.27, 'throughput': 2792.74}\n","{'loss': 0.3431, 'grad_norm': 2.4427571296691895, 'learning_rate': 4.785991076286141e-06, 'epoch': 2.27, 'num_input_tokens_seen': 9827840}\n"," 81% 375/462 [1:02:56<15:15, 10.52s/it][INFO|2025-06-14 13:07:45] llamafactory.train.callbacks:143 >> {'loss': 0.3394, 'learning_rate': 2.9682e-06, 'epoch': 2.43, 'throughput': 2793.25}\n","{'loss': 0.3394, 'grad_norm': 2.233779191970825, 'learning_rate': 2.9681521086743426e-06, 'epoch': 2.43, 'num_input_tokens_seen': 10547904}\n"," 87% 400/462 [1:07:05<09:59,  9.66s/it][INFO|2025-06-14 13:11:54] llamafactory.train.callbacks:143 >> {'loss': 0.3494, 'learning_rate': 1.5469e-06, 'epoch': 2.60, 'throughput': 2793.57}\n","{'loss': 0.3494, 'grad_norm': 2.646268844604492, 'learning_rate': 1.5469088770096762e-06, 'epoch': 2.6, 'num_input_tokens_seen': 11245504}\n"," 87% 400/462 [1:07:05<09:59,  9.66s/it][INFO|trainer.py:3984] 2025-06-14 13:11:54,416 >> Saving model checkpoint to saves/Llama-3-8B/lora/train_2025-06-14-12-02-52_top_5/checkpoint-400\n","[INFO|configuration_utils.py:693] 2025-06-14 13:11:54,599 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/config.json\n","[INFO|configuration_utils.py:765] 2025-06-14 13:11:54,600 >> Model config LlamaConfig {\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.51.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2510] 2025-06-14 13:11:55,370 >> tokenizer config file saved in saves/Llama-3-8B/lora/train_2025-06-14-12-02-52_top_5/checkpoint-400/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2519] 2025-06-14 13:11:55,375 >> Special tokens file saved in saves/Llama-3-8B/lora/train_2025-06-14-12-02-52_top_5/checkpoint-400/special_tokens_map.json\n"," 92% 425/462 [1:11:22<06:12, 10.07s/it][INFO|2025-06-14 13:16:11] llamafactory.train.callbacks:143 >> {'loss': 0.3402, 'learning_rate': 5.6911e-07, 'epoch': 2.76, 'throughput': 2791.97}\n","{'loss': 0.3402, 'grad_norm': 2.654282808303833, 'learning_rate': 5.691086328958028e-07, 'epoch': 2.76, 'num_input_tokens_seen': 11955456}\n"," 97% 450/462 [1:15:37<02:01, 10.10s/it][INFO|2025-06-14 13:20:26] llamafactory.train.callbacks:143 >> {'loss': 0.3408, 'learning_rate': 6.6982e-08, 'epoch': 2.92, 'throughput': 2792.27}\n","{'loss': 0.3408, 'grad_norm': 2.218446969985962, 'learning_rate': 6.698178773139108e-08, 'epoch': 2.92, 'num_input_tokens_seen': 12669248}\n","100% 462/462 [1:17:37<00:00, 10.27s/it][INFO|trainer.py:3984] 2025-06-14 13:22:26,837 >> Saving model checkpoint to saves/Llama-3-8B/lora/train_2025-06-14-12-02-52_top_5/checkpoint-462\n","[INFO|configuration_utils.py:693] 2025-06-14 13:22:27,058 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/config.json\n","[INFO|configuration_utils.py:765] 2025-06-14 13:22:27,059 >> Model config LlamaConfig {\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.51.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2510] 2025-06-14 13:22:27,860 >> tokenizer config file saved in saves/Llama-3-8B/lora/train_2025-06-14-12-02-52_top_5/checkpoint-462/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2519] 2025-06-14 13:22:27,866 >> Special tokens file saved in saves/Llama-3-8B/lora/train_2025-06-14-12-02-52_top_5/checkpoint-462/special_tokens_map.json\n","[INFO|trainer.py:2681] 2025-06-14 13:22:32,426 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 4663.5165, 'train_samples_per_second': 3.179, 'train_steps_per_second': 0.099, 'train_loss': 0.60948257374041, 'epoch': 3.0, 'num_input_tokens_seen': 13005760}\n","100% 462/462 [1:17:43<00:00, 10.09s/it]\n","[INFO|trainer.py:3984] 2025-06-14 13:22:32,433 >> Saving model checkpoint to saves/Llama-3-8B/lora/train_2025-06-14-12-02-52_top_5\n","[INFO|configuration_utils.py:693] 2025-06-14 13:22:32,589 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/config.json\n","[INFO|configuration_utils.py:765] 2025-06-14 13:22:32,590 >> Model config LlamaConfig {\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.51.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2510] 2025-06-14 13:22:33,318 >> tokenizer config file saved in saves/Llama-3-8B/lora/train_2025-06-14-12-02-52_top_5/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2519] 2025-06-14 13:22:33,322 >> Special tokens file saved in saves/Llama-3-8B/lora/train_2025-06-14-12-02-52_top_5/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =      2.9968\n","  num_input_tokens_seen    =    13005760\n","  total_flos               = 551519457GF\n","  train_loss               =      0.6095\n","  train_runtime            =  1:17:43.51\n","  train_samples_per_second =       3.179\n","  train_steps_per_second   =       0.099\n","Figure saved at: saves/Llama-3-8B/lora/train_2025-06-14-12-02-52_top_5/training_loss.png\n","[WARNING|2025-06-14 13:22:33] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n","[WARNING|2025-06-14 13:22:33] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n","[INFO|modelcard.py:450] 2025-06-14 13:22:33,738 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n","2025-06-14 13:23:50.147730: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1749907430.169074   42245 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1749907430.175610   42245 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","[INFO|2025-06-14 13:23:57] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: None\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:23:57,616 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:23:57,617 >> loading file tokenizer.model from cache at None\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:23:57,617 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:23:57,617 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:23:57,617 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:23:57,617 >> loading file chat_template.jinja from cache at None\n","[INFO|tokenization_utils_base.py:2323] 2025-06-14 13:23:58,106 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|configuration_utils.py:693] 2025-06-14 13:23:58,449 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/config.json\n","[INFO|configuration_utils.py:765] 2025-06-14 13:23:58,451 >> Model config LlamaConfig {\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.51.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:23:58,517 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:23:58,517 >> loading file tokenizer.model from cache at None\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:23:58,517 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:23:58,517 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:23:58,517 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:23:58,517 >> loading file chat_template.jinja from cache at None\n","[INFO|tokenization_utils_base.py:2323] 2025-06-14 13:23:59,021 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|2025-06-14 13:23:59] llamafactory.data.template:143 >> Add pad token: <|end_of_text|>\n","[INFO|2025-06-14 13:23:59] llamafactory.data.loader:143 >> Loading dataset top_5_testing_dataset.json...\n","eval example:\n","input_ids:\n","[39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 2675, 527, 264, 7452, 4877, 34465, 627, 5715, 27785, 279, 8581, 9681, 505, 420, 8521, 1160, 512, 681, 15893, 9497, 518, 364, 10590, 518, 364, 14706, 518, 364, 9996, 518, 364, 695, 14726, 4532, 5207, 7041, 279, 9681, 11, 43147, 11, 19180, 555, 32783, 29466, 29047, 13, 2360, 1023, 4339, 627, 4959, 1988, 17610, 315, 512, 482, 362, 15840, 10039, 10329, 5224, 627, 482, 1556, 76994, 66233, 12730, 1, 3857, 430, 15100, 279, 10825, 6425, 627, 10464, 86589, 5224, 11, 323, 28144, 311, 10491, 9681, 627, 14711, 67346, 220, 16, 720, 32298, 25, 16644, 459, 1358, 315, 308, 26864, 11, 4320, 2874, 2134, 8187, 20126, 627, 9597, 532, 25, 5075, 4734, 449, 264, 34544, 2007, 304, 507, 1471, 1515, 308, 705, 1243, 4320, 304, 507, 7, 16, 4390, 16533, 25, 828, 14726, 198, 14711, 67346, 220, 17, 720, 32298, 25, 1472, 617, 452, 19289, 323, 2218, 2694, 328, 13, 5560, 1855, 16652, 520, 1455, 3131, 627, 9597, 532, 25, 8012, 11581, 1004, 1483, 73, 60, 284, 69384, 37498, 1701, 1176, 602, 19289, 627, 16533, 25, 11581, 271, 7184, 49229, 512, 65454, 9186, 5602, 264, 3544, 7698, 5439, 389, 279, 5015, 323, 6944, 311, 1194, 1202, 8026, 1376, 704, 18308, 9186, 706, 27332, 279, 961, 315, 13353, 1139, 1202, 8026, 3645, 4800, 568, 374, 2133, 311, 1194, 433, 304, 279, 2768, 11827, 1283, 706, 459, 7698, 323, 649, 1193, 2804, 279, 2768, 1403, 7677, 304, 904, 2015, 369, 27862, 3115, 1855, 8377, 308, 304, 8026, 1376, 2085, 6522, 17975, 1855, 1194, 690, 8911, 311, 279, 1314, 315, 3766, 24370, 40733, 308, 555, 220, 16, 6914, 274, 7124, 459, 439, 264, 8668, 315, 7677, 430, 649, 7946, 1194, 8026, 13340, 315, 2085, 6522, 17975, 323, 10548, 449, 264, 1194, 5784, 602, 384, 5784, 220, 16, 18308, 9186, 6944, 311, 1440, 1268, 1690, 2204, 10728, 24630, 527, 1070, 323, 279, 3160, 304, 7677, 315, 279, 40243, 10728, 8668, 578, 11503, 2643, 387, 3544, 779, 4587, 1194, 1124, 60964, 220, 1041, 931, 931, 22, 6914, 274, 7124, 279, 925, 13340, 315, 459, 10728, 8668, 439, 264, 925, 315, 323, 1405, 279, 270, 3752, 304, 279, 925, 9248, 279, 270, 5784, 10887, 9220, 10728, 24630, 527, 2663, 2204, 422, 872, 925, 44713, 527, 2204, 198, 4521, 66233, 55023, 4516, 1095, 274, 1427, 1139, 1268, 311, 11294, 279, 907, 369, 264, 2728, 6857, 1442, 279, 907, 315, 374, 6273, 311, 477, 7191, 1109, 1109, 374, 1606, 422, 374, 2753, 1109, 1053, 1304, 3160, 315, 279, 502, 17071, 2753, 1109, 279, 3766, 17071, 8617, 1202, 907, 1053, 387, 32415, 1628, 422, 649, 259, 387, 279, 907, 315, 374, 2744, 264, 2764, 5873, 1606, 433, 1053, 1304, 279, 3160, 315, 279, 502, 17071, 7191, 1109, 279, 3766, 832, 2100, 369, 1855, 3160, 422, 584, 1440, 279, 2015, 315, 323, 304, 892, 584, 649, 11294, 597, 304, 892, 649, 387, 6847, 6982, 555, 26619, 3861, 1648, 315, 3815, 1778, 374, 1701, 9436, 60115, 12384, 369, 21166, 1358, 8246, 311, 1977, 264, 31915, 48, 6070, 369, 3319, 304, 892, 578, 9436, 60115, 12384, 7612, 864, 28806, 892, 7181, 1070, 374, 2103, 264, 5370, 315, 5627, 311, 656, 420, 961, 315, 3465, 304, 279, 1890, 477, 2731, 892, 1946, 2302, 552, 1628, 369, 279, 40243, 3160, 961, 584, 649, 12849, 279, 17832, 5596, 4460, 779, 3117, 369, 1855, 1614, 3235, 449, 279, 864, 344, 788, 11581, 5112, 9616, 682, 5415, 10548, 449, 28993, 584, 649, 11886, 420, 3575, 304, 449, 28930, 304, 3649, 1093, 23546, 323, 4793, 7677, 892, 23965, 7181, 279, 6205, 2082, 1005, 264, 20829, 5010, 75, 5010, 2373, 315, 9436, 60115, 12384, 6205, 2082, 220, 23282, 20767, 21, 271, 14711, 6075, 512]\n","inputs:\n","Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","You are a strict tag classifier.\n","Return ONLY the applicable tags from this fixed list:\n","['greedy','math', 'implementation', 'dp', 'data structures']\n","Output exactly the tags, lowercase, separated by comma-no-space. No other words.\n","Each input consists of:\n"," - A programming-problem statement.\n"," - An \"--- Editorial ---\" section that explains the intended solution.\n","Use BOTH statement, and editorial to decide tags.\n","### EXAMPLE 1 \n","Problem: Given an array of n integers, answer q range minimum queries.\n","Editorial: Preprocess with a sparse table in O(n log n), then answer in O(1).\n","Answer: data structures\n","### EXAMPLE 2 \n","Problem: You have N coins and target sum S. Use each coin at most once.\n","Editorial: Build dp[i][j] = reachable sums using first i coins.\n","Answer: dp\n","\n","Now classify:\n","Dreamoon saw a large integer written on the ground and wants to print its binary form out Dreamoon has accomplished the part of turning into its binary format Now he is going to print it in the following manner He has an integer and can only perform the following two operations in any order for unlimited times each Print n in binary form without leading zeros each print will append to the right of previous prints Increase n by 1 Let s define an as a sequence of operations that can successfully print binary representation of without leading zeros and ends with a print operation i e operation 1 Dreamoon wants to know how many different ideal sequences are there and the length in operations of the shortest ideal sequence The answers might be large so please print them modulo 1000000007 Let s define the string representation of an ideal sequence as a string of and where the th character in the string matches the th operation performed Two ideal sequences are called different if their string representations are different\n","--- Editorial ---\n","So let s look into how to calculate the value for a given pair If the value of is equal to or greater than than is because if is less than would make length of the new partition less than the previous partition thus its value would be lesser And if can t be the value of is always a valid choice because it would make the length of the new partition greater than the previous one So for each length if we know the order of and in time we can calculate k in time can be easily shown by assuming One way of doing such is using prefix doubling algorithm for suffix array construction to build a RMQ structure for query in time The prefix doubling algorithm requires precompute time Note there is still a various of ways to do this part of task in the same or better time complexties And for the shortest length part we can compute the minimal parts needed so far for each state along with the preivous dp Then compare all states ends with Overall we can solve this problem in with caution in details like boundaries and module operations time complexity Note the sample code use a nlgnlgn version of prefix doubling algorithm sample code 8215216\n","\n","### Response:\n","\n","label_ids:\n","[9996, 128001, 271]\n","labels:\n","dp<|end_of_text|>\n","\n","\n","[INFO|configuration_utils.py:693] 2025-06-14 13:23:59,560 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/config.json\n","[INFO|configuration_utils.py:765] 2025-06-14 13:23:59,561 >> Model config LlamaConfig {\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.51.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|2025-06-14 13:23:59] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n","[INFO|modeling_utils.py:1124] 2025-06-14 13:23:59,616 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/model.safetensors.index.json\n","[INFO|modeling_utils.py:2167] 2025-06-14 13:23:59,617 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n","[INFO|configuration_utils.py:1142] 2025-06-14 13:23:59,619 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001\n","}\n","\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.17s/it]\n","[INFO|modeling_utils.py:4930] 2025-06-14 13:24:04,347 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","[INFO|modeling_utils.py:4938] 2025-06-14 13:24:04,347 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","[INFO|configuration_utils.py:1097] 2025-06-14 13:24:04,415 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/generation_config.json\n","[INFO|configuration_utils.py:1142] 2025-06-14 13:24:04,416 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": 128001,\n","  \"max_length\": 4096,\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","[INFO|2025-06-14 13:24:04] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n","[INFO|2025-06-14 13:24:06] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n","[INFO|2025-06-14 13:24:06] llamafactory.model.adapter:143 >> Loaded adapter(s): saves/Llama-3-8B/lora/train_2025-06-14-12-02-52_top_5\n","[INFO|2025-06-14 13:24:06] llamafactory.model.loader:143 >> all params: 8,030,261,248\n","[WARNING|2025-06-14 13:24:06] llamafactory.train.sft.workflow:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.\n","[INFO|trainer.py:4307] 2025-06-14 13:24:06,277 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4309] 2025-06-14 13:24:06,277 >>   Num examples = 1249\n","[INFO|trainer.py:4312] 2025-06-14 13:24:06,277 >>   Batch size = 32\n","100% 40/40 [01:53<00:00,  2.21s/it]Building prefix dict from the default dictionary ...\n","DEBUG:jieba:Building prefix dict from the default dictionary ...\n","Loading model from cache /tmp/jieba.cache\n","DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n","Loading model cost 0.794 seconds.\n","DEBUG:jieba:Loading model cost 0.794 seconds.\n","Prefix dict has been built successfully.\n","DEBUG:jieba:Prefix dict has been built successfully.\n","100% 40/40 [01:55<00:00,  2.88s/it]\n","***** predict metrics *****\n","  predict_bleu-4                 =    42.3803\n","  predict_model_preparation_time =     0.0047\n","  predict_rouge-1                =    66.0472\n","  predict_rouge-2                =    23.2404\n","  predict_rouge-l                =    64.0395\n","  predict_runtime                = 0:01:58.81\n","  predict_samples_per_second     =     10.512\n","  predict_steps_per_second       =      0.337\n","[INFO|2025-06-14 13:26:05] llamafactory.train.sft.trainer:143 >> Saving prediction results to saves/Llama-3-8B/lora/eval_2025-06-14-12-02-52_top_5/generated_predictions.jsonl\n","2025-06-14 13:26:57.773531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1749907617.795270   43103 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1749907617.801975   43103 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","[INFO|2025-06-14 13:27:05] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: None\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:27:05,195 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:27:05,196 >> loading file tokenizer.model from cache at None\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:27:05,196 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:27:05,196 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:27:05,196 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:27:05,196 >> loading file chat_template.jinja from cache at None\n","[INFO|tokenization_utils_base.py:2323] 2025-06-14 13:27:05,688 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|configuration_utils.py:693] 2025-06-14 13:27:05,954 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/config.json\n","[INFO|configuration_utils.py:765] 2025-06-14 13:27:05,957 >> Model config LlamaConfig {\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.51.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:27:06,024 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:27:06,024 >> loading file tokenizer.model from cache at None\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:27:06,024 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:27:06,024 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:27:06,024 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2060] 2025-06-14 13:27:06,024 >> loading file chat_template.jinja from cache at None\n","[INFO|tokenization_utils_base.py:2323] 2025-06-14 13:27:06,510 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|2025-06-14 13:27:06] llamafactory.data.template:143 >> Add pad token: <|end_of_text|>\n","[INFO|2025-06-14 13:27:06] llamafactory.data.loader:143 >> Loading dataset top_5_testing_dataset_2025.json...\n","Running tokenizer on dataset (num_proc=16): 100% 37/37 [00:04<00:00,  7.85 examples/s]\n","eval example:\n","input_ids:\n","[39314, 374, 459, 7754, 430, 16964, 264, 3465, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 2675, 527, 264, 7452, 4877, 34465, 627, 5715, 27785, 279, 8581, 9681, 505, 420, 8521, 1160, 512, 681, 15893, 9497, 518, 364, 10590, 518, 364, 14706, 518, 364, 9996, 518, 364, 695, 14726, 4532, 5207, 7041, 279, 9681, 11, 43147, 11, 19180, 555, 32783, 29466, 29047, 13, 2360, 1023, 4339, 627, 4959, 1988, 17610, 315, 512, 482, 362, 15840, 10039, 10329, 5224, 627, 482, 1556, 76994, 66233, 12730, 1, 3857, 430, 15100, 279, 10825, 6425, 627, 10464, 86589, 5224, 11, 323, 28144, 311, 10491, 9681, 627, 14711, 67346, 220, 16, 720, 32298, 25, 16644, 459, 1358, 315, 308, 26864, 11, 4320, 2874, 2134, 8187, 20126, 627, 9597, 532, 25, 5075, 4734, 449, 264, 34544, 2007, 304, 507, 1471, 1515, 308, 705, 1243, 4320, 304, 507, 7, 16, 4390, 16533, 25, 828, 14726, 198, 14711, 67346, 220, 17, 720, 32298, 25, 1472, 617, 452, 19289, 323, 2218, 2694, 328, 13, 5560, 1855, 16652, 520, 1455, 3131, 627, 9597, 532, 25, 8012, 11581, 1004, 1483, 73, 60, 284, 69384, 37498, 1701, 1176, 602, 19289, 627, 16533, 25, 11581, 271, 7184, 49229, 512, 10267, 274, 1650, 459, 7698, 8668, 422, 279, 2768, 4787, 3412, 1202, 3160, 374, 520, 3325, 220, 18, 369, 1475, 2449, 3734, 279, 1176, 832, 1070, 374, 459, 2449, 311, 279, 2163, 2753, 1109, 433, 369, 1475, 2449, 3734, 279, 1566, 832, 1070, 374, 459, 2449, 311, 279, 1314, 8294, 1109, 433, 1789, 3187, 220, 16, 220, 19, 220, 17, 220, 19, 220, 22, 323, 220, 16, 220, 17, 220, 19, 220, 23, 527, 6366, 719, 220, 16, 220, 17, 220, 17, 220, 17, 220, 19, 323, 220, 16, 220, 18, 220, 20, 220, 18, 527, 539, 80640, 430, 264, 1207, 15880, 374, 264, 8668, 430, 649, 387, 12457, 505, 2500, 8668, 555, 18054, 1063, 5540, 2085, 10223, 279, 2015, 315, 279, 9861, 5540, 1472, 527, 2728, 459, 7698, 1358, 264, 315, 1404, 308, 1405, 4718, 3465, 374, 311, 11294, 279, 1396, 315, 6366, 13386, 2436, 315, 279, 1358, 264, 8876, 279, 4320, 2643, 387, 3544, 1194, 433, 60964, 220, 19416, 13719, 17228, 198, 4521, 66233, 55023, 10267, 274, 2349, 279, 7419, 315, 264, 6366, 8668, 264, 2766, 1226, 649, 12391, 430, 279, 3044, 369, 1475, 2449, 3734, 279, 1176, 832, 1070, 374, 459, 2449, 311, 279, 2163, 2753, 1109, 433, 374, 13890, 311, 279, 1176, 2449, 374, 2753, 1109, 1475, 1023, 2449, 304, 279, 8668, 1226, 649, 12391, 430, 1521, 1403, 4787, 527, 13890, 555, 38156, 369, 279, 15953, 2449, 315, 279, 8668, 279, 1193, 2449, 311, 279, 2163, 374, 279, 357, 779, 9855, 584, 617, 19168, 430, 369, 1475, 6914, 274, 12391, 430, 83710, 430, 279, 2449, 311, 279, 2163, 315, 902, 374, 2753, 1109, 374, 422, 1243, 14224, 6062, 323, 779, 12362, 4528, 38156, 584, 649, 12391, 430, 369, 1475, 2449, 3734, 279, 1566, 832, 1070, 374, 459, 2449, 311, 279, 1314, 8294, 1109, 433, 374, 279, 1890, 439, 279, 1566, 2449, 374, 7191, 1109, 1475, 1023, 2449, 304, 279, 8668, 8876, 279, 1358, 17610, 1193, 315, 323, 584, 649, 5406, 430, 1070, 374, 1193, 832, 3284, 6366, 8668, 5497, 602, 384, 832, 8272, 555, 904, 1396, 315, 24871, 323, 832, 1620, 5884, 1023, 5497, 374, 8482, 279, 2163, 3646, 2449, 1288, 387, 26549, 2753, 1109, 1475, 2449, 304, 279, 6278, 1475, 2449, 505, 279, 15953, 311, 279, 2132, 311, 1566, 323, 279, 1314, 3646, 1288, 387, 26549, 7191, 1109, 1475, 2449, 304, 279, 6278, 779, 279, 2163, 3646, 2449, 1288, 387, 279, 1314, 3646, 2449, 1288, 387, 323, 1475, 2449, 304, 279, 6278, 1288, 387, 763, 2015, 311, 11294, 279, 1396, 315, 13386, 2436, 430, 2489, 279, 46752, 5497, 584, 649, 1005, 8915, 15840, 6914, 387, 279, 1396, 315, 13386, 2436, 422, 584, 617, 6646, 279, 1176, 5540, 315, 279, 1358, 323, 279, 1510, 1614, 374, 369, 3187, 1614, 3445, 430, 279, 8668, 374, 4384, 1614, 3445, 430, 584, 617, 4529, 279, 2449, 6273, 311, 1614, 3445, 430, 584, 617, 4529, 1063, 1396, 315, 274, 1614, 3445, 430, 584, 617, 4529, 279, 2449, 323, 279, 8668, 374, 8220, 578, 34692, 304, 420, 8915, 15840, 527, 5128, 4382, 323, 649, 387, 2884, 304, 369, 1855, 1614, 2100, 279, 2860, 23965, 315, 279, 6425, 374, 271, 14711, 6075, 512]\n","inputs:\n","Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","You are a strict tag classifier.\n","Return ONLY the applicable tags from this fixed list:\n","['greedy','math', 'implementation', 'dp', 'data structures']\n","Output exactly the tags, lowercase, separated by comma-no-space. No other words.\n","Each input consists of:\n"," - A programming-problem statement.\n"," - An \"--- Editorial ---\" section that explains the intended solution.\n","Use BOTH statement, and editorial to decide tags.\n","### EXAMPLE 1 \n","Problem: Given an array of n integers, answer q range minimum queries.\n","Editorial: Preprocess with a sparse table in O(n log n), then answer in O(1).\n","Answer: data structures\n","### EXAMPLE 2 \n","Problem: You have N coins and target sum S. Use each coin at most once.\n","Editorial: Build dp[i][j] = reachable sums using first i coins.\n","Answer: dp\n","\n","Now classify:\n","Let s call an integer sequence if the following conditions hold its length is at least 3 for every element except the first one there is an element to the left less than it for every element except the last one there is an element to the right larger than it For example 1 4 2 4 7 and 1 2 4 8 are beautiful but 1 2 2 2 4 and 1 3 5 3 are not Recall that a subsequence is a sequence that can be obtained from another sequence by removing some elements without changing the order of the remaining elements You are given an integer array a of size n where Your task is to calculate the number of beautiful subsequences of the array a Since the answer might be large print it modulo 998244353\n","--- Editorial ---\n","Let s change the definition of a beautiful sequence a bit We can prove that the condition for every element except the first one there is an element to the left less than it is equivalent to the first element is less than every other element in the sequence We can prove that these two conditions are equivalent by induction for the nd element of the sequence the only element to the left is the st so assume we have proved that for every Let s prove that Suppose that the element to the left of which is less than is if then obviously otherwise and so Using similar induction we can prove that for every element except the last one there is an element to the right larger than it is the same as the last element is greater than every other element in the sequence Since the array consists only of and we can notice that there is only one possible beautiful sequence pattern i e one followed by any number of consecutive and one final Any other pattern is invalid the leftmost element should be strictly less than every element in the middle every element from the nd to the second to last and the rightmost should be strictly greater than every element in the middle so the leftmost element should be the rightmost element should be and every element in the middle should be In order to calculate the number of subsequences that match the aforementioned pattern we can use dynamic programming Let be the number of subsequences if we have considered the first elements of the array and the current state is for example state means that the sequence is empty state means that we have taken the element equal to state means that we have taken some number of s state means that we have taken the element and the sequence is finished The transitions in this dynamic programming are pretty simple and can be done in for each state So the total complexity of the solution is\n","\n","### Response:\n","\n","label_ids:\n","[9996, 11, 57080, 128001, 271]\n","labels:\n","dp, greedy<|end_of_text|>\n","\n","\n","[INFO|configuration_utils.py:693] 2025-06-14 13:27:12,815 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/config.json\n","[INFO|configuration_utils.py:765] 2025-06-14 13:27:12,816 >> Model config LlamaConfig {\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001,\n","  \"head_dim\": 128,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 8192,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.51.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","[INFO|2025-06-14 13:27:12] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n","[INFO|modeling_utils.py:1124] 2025-06-14 13:27:12,877 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/model.safetensors.index.json\n","[INFO|modeling_utils.py:2167] 2025-06-14 13:27:12,878 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n","[INFO|configuration_utils.py:1142] 2025-06-14 13:27:12,880 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128001\n","}\n","\n","Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.17s/it]\n","[INFO|modeling_utils.py:4930] 2025-06-14 13:27:17,605 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","[INFO|modeling_utils.py:4938] 2025-06-14 13:27:17,605 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","[INFO|configuration_utils.py:1097] 2025-06-14 13:27:17,674 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/generation_config.json\n","[INFO|configuration_utils.py:1142] 2025-06-14 13:27:17,675 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": 128001,\n","  \"max_length\": 4096,\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","[INFO|2025-06-14 13:27:17] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n","[INFO|2025-06-14 13:27:19] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n","[INFO|2025-06-14 13:27:19] llamafactory.model.adapter:143 >> Loaded adapter(s): saves/Llama-3-8B/lora/train_2025-06-14-12-02-52_top_5\n","[INFO|2025-06-14 13:27:19] llamafactory.model.loader:143 >> all params: 8,030,261,248\n","[WARNING|2025-06-14 13:27:19] llamafactory.train.sft.workflow:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.\n","[INFO|trainer.py:4307] 2025-06-14 13:27:19,543 >> \n","***** Running Prediction *****\n","[INFO|trainer.py:4309] 2025-06-14 13:27:19,543 >>   Num examples = 37\n","[INFO|trainer.py:4312] 2025-06-14 13:27:19,543 >>   Batch size = 32\n","100% 2/2 [00:00<00:00,  3.48it/s]Building prefix dict from the default dictionary ...\n","DEBUG:jieba:Building prefix dict from the default dictionary ...\n","Loading model from cache /tmp/jieba.cache\n","DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n","Loading model cost 0.803 seconds.\n","DEBUG:jieba:Loading model cost 0.803 seconds.\n","Prefix dict has been built successfully.\n","DEBUG:jieba:Prefix dict has been built successfully.\n","100% 2/2 [00:01<00:00,  1.42it/s]\n","***** predict metrics *****\n","  predict_bleu-4                 =    38.3201\n","  predict_model_preparation_time =     0.0048\n","  predict_rouge-1                =     67.677\n","  predict_rouge-2                =    20.2467\n","  predict_rouge-l                =    57.7228\n","  predict_runtime                = 0:00:04.91\n","  predict_samples_per_second     =      7.536\n","  predict_steps_per_second       =      0.407\n","[INFO|2025-06-14 13:27:24] llamafactory.train.sft.trainer:143 >> Saving prediction results to saves/Llama-3-8B/lora/eval_2025-06-14-12-02-52_top_5_2025/generated_predictions.jsonl\n","Keyboard interruption in main thread... closing server.\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2997, in block_thread\n","    time.sleep(0.1)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier/LLaMA-Factory/src/webui.py\", line 31, in <module>\n","    main()\n","  File \"/content/drive/Othercomputers/ASUS Main/MLCP/03_Classifier/LLaMA-Factory/src/webui.py\", line 27, in main\n","    create_ui().queue().launch(share=True, server_name=server_name, inbrowser=True)\n","  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2903, in launch\n","    self.block_thread()\n","  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 3001, in block_thread\n","    self.server.close()\n","  File \"/usr/local/lib/python3.11/dist-packages/gradio/http_server.py\", line 69, in close\n","    self.thread.join(timeout=5)\n","  File \"/usr/lib/python3.11/threading.py\", line 1087, in join\n","    def join(self, timeout=None):\n","\n","KeyboardInterrupt\n","Killing tunnel 0.0.0.0:7860 <> https://e3780f88b84b0e97c9.gradio.live\n"]}]},{"cell_type":"code","execution_count":null,"id":"9a402ef6","metadata":{"id":"9a402ef6"},"outputs":[],"source":["import ast\n","import math\n","from typing import List, Sequence, Union\n","\n","TOP_5_OUR_DATASET_TAGS = \"['data structures', 'greedy', 'math', 'implementation', 'dp']\"\n","TOP_10_OUR_DATASET_TAGS = \"['greedy', 'dp', 'graphs', 'brute force', 'math', 'constructive algorithms', 'sortings', 'implementation', 'binary search', 'data structures']\"\n","TOP_20_OUR_DATASET_TAGS = \"['implementation', 'binary search', 'math', 'number theory', 'greedy', 'graphs', 'data structures', 'geometry', 'sortings', 'dp', 'brute force', 'combinatorics', 'dfs and similar', 'constructive algorithms', 'trees', 'strings', 'two pointers', 'dsu', 'bitmasks', 'divide and conquer']\"\n","\n","# ALWAYS UPDATE TAG_LIST\n","TAG_LIST = TOP_5_OUR_DATASET_TAGS\n","\n","def _to_list(value: Union[str, Sequence[str], None]) -> List[str]:\n","    \"\"\"Convert any tag representation  a list of clean tag strings.\"\"\"\n","    if value is None:\n","        return []\n","    # Pandas NaN is float and math.isnan() is True\n","    if isinstance(value, float) and math.isnan(value):\n","        return []\n","    # Already a Python list\n","    if isinstance(value, (list, tuple)):\n","        return [str(t).strip() for t in value if str(t).strip()]\n","    # Make it a trimmed string and strip line-breaks\n","    value = str(value).strip().replace(\"\\n\", \"\")\n","    if value == \"\":\n","        return []\n","    # Case 1: looks like JSON / Python list\n","    if value.startswith(\"[\") and value.endswith(\"]\"):\n","        try:\n","            return [str(t).strip() for t in ast.literal_eval(value)]\n","        except Exception:\n","            pass  # fall through to comma split\n","    # Case 2: plain comma-separated line\n","    return [t.strip() for t in value.split(\",\") if t.strip()]\n","\n","def create_binary_vector(tag_like, unique_tags=TAG_LIST) -> List[int]:\n","    \"\"\"Return multi-hot vector the length of `unique_tags`.\"\"\"\n","    # ensure unique_tags is a list\n","    if isinstance(unique_tags, str):\n","        unique_tags = ast.literal_eval(unique_tags)\n","    tags = _to_list(tag_like)\n","    vector = [0] * len(unique_tags)\n","    for tag in tags:\n","        if tag in unique_tags:\n","            vector[unique_tags.index(tag)] = 1\n","    return vector\n"]},{"cell_type":"code","execution_count":null,"id":"c6608cb0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1749907722403,"user":{"displayName":"Dinu Ion George","userId":"14034259797438922081"},"user_tz":-180},"id":"c6608cb0","outputId":"c0060741-3477-4953-8729-fb6611a0d15c"},"outputs":[{"output_type":"stream","name":"stdout","text":["F1 Macro Score: 0.6718207282913166\n","AUROC: 0.7875128635983899\n"]}],"source":["\n","from sklearn.metrics import f1_score, roc_auc_score\n","import pandas as pd\n","import json\n","import os\n","\n","generated_results = []\n","with open(\"saves/Llama-3-8B/lora/eval_2025-06-14-12-02-52_top_5_2025/generated_predictions.jsonl\", \"r\") as file:\n","    for line in file:\n","        generated_results.append(json.loads(line))\n","\n","truths = []\n","predictions = []\n","\n","# Iterate through the test dataset and generated results\n","for data in generated_results:\n","    # Extract the relevant fields from the test dataset\n","    truths.append(data[\"label\"])\n","    predictions.append(data[\"predict\"])\n","\n","\n","merged_df = pd.DataFrame({\n","    'truths': truths,\n","    'predictions': predictions\n","})\n","\n","merged_df['truths'] = merged_df['truths'].apply(lambda x: create_binary_vector(x, TAG_LIST))\n","merged_df['predictions'] = merged_df['predictions'].apply(lambda x: create_binary_vector(x, TAG_LIST))\n","\n","# Calculate F1 macro score\n","f1_macro = f1_score(merged_df['truths'].tolist(), merged_df['predictions'].tolist(), average='macro')\n","\n","# Calculate AUROC\n","auroc = roc_auc_score(merged_df['truths'].tolist(), merged_df['predictions'].tolist(), average='macro')\n","\n","print(f\"F1 Macro Score: {f1_macro}\")\n","print(f\"AUROC: {auroc}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}